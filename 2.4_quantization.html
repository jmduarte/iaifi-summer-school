
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Part 4: Quantization &#8212; My sample book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Part 3: Compression" href="2.3_compression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">My sample book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1.1_tabular_data_efps.html">
   Lab 1: Jets as Tabular Data using Energy Flow Polynomials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.3_deep_sets.html">
   Deep Sets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.4_gnn_in.html">
   Interaction Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.5_lorentz_network.html">
   Lorentz Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2.1_getting_started.html">
   Part 1: Getting started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2.2_advanced_config.html">
   Part 2: Advanced Configuration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2.3_compression.html">
   Part 3: Compression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Part 4: Quantization
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/2.4_quantization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F2.4_quantization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/2.4_quantization.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Part 4: Quantization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fetch-the-jet-tagging-dataset-from-open-ml">
     Fetch the jet tagging dataset from Open ML
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#construct-a-model">
     Construct a model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-sparse">
     Train sparse
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-the-model">
     Train the model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#check-performance">
     Check performance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synthesize">
   Synthesize
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#check-the-reports">
     Check the reports
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nb">
     NB
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Part 4: Quantization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Part 4: Quantization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fetch-the-jet-tagging-dataset-from-open-ml">
     Fetch the jet tagging dataset from Open ML
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#construct-a-model">
     Construct a model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-sparse">
     Train sparse
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-the-model">
     Train the model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#check-performance">
     Check performance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synthesize">
   Synthesize
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#check-the-reports">
     Check the reports
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nb">
     NB
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="part-4-quantization">
<h1>Part 4: Quantization<a class="headerlink" href="#part-4-quantization" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="c1">#import os</span>
<span class="c1">#os.environ[&#39;PATH&#39;] = &#39;/opt/Xilinx/Vivado/2019.2/bin:&#39; + os.environ[&#39;PATH&#39;]</span>
<span class="c1"># for this tutorial we wont be actually running Vivado, so I have commented these lines out</span>
<span class="c1">#     but if you want to look into actually running on an FPGA then simply uncomment these lines</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-07-31 02:16:40.668814: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.7.13/x64/lib
2022-07-31 02:16:40.668847: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
</pre></div>
</div>
</div>
</div>
<section id="fetch-the-jet-tagging-dataset-from-open-ml">
<h2>Fetch the jet tagging dataset from Open ML<a class="headerlink" href="#fetch-the-jet-tagging-dataset-from-open-ml" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;X_train_val.npy&#39;</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;X_test.npy&#39;</span><span class="p">)</span>
<span class="n">y_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;y_train_val.npy&#39;</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;y_test.npy&#39;</span><span class="p">)</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;classes.npy&#39;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_4985</span><span class="o">/</span><span class="mf">2878572050.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">X_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;X_train_val.npy&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;X_test.npy&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">y_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;y_train_val.npy&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;y_test.npy&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;classes.npy&#39;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/numpy/lib/npyio.py</span> in <span class="ni">load</span><span class="nt">(file, mmap_mode, allow_pickle, fix_imports, encoding)</span>
<span class="g g-Whitespace">    </span><span class="mi">415</span>             <span class="n">own_fid</span> <span class="o">=</span> <span class="kc">False</span>
<span class="g g-Whitespace">    </span><span class="mi">416</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">417</span>             <span class="n">fid</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">os_fspath</span><span class="p">(</span><span class="n">file</span><span class="p">),</span> <span class="s2">&quot;rb&quot;</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">418</span>             <span class="n">own_fid</span> <span class="o">=</span> <span class="kc">True</span>
<span class="g g-Whitespace">    </span><span class="mi">419</span> 

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;X_train_val.npy&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="construct-a-model">
<h2>Construct a model<a class="headerlink" href="#construct-a-model" title="Permalink to this headline">#</a></h2>
<p>This time we’re going to use QKeras layers.
QKeras is “Quantized Keras” for deep heterogeneous quantization of ML models.</p>
<p><a class="reference external" href="https://github.com/google/qkeras">https://github.com/google/qkeras</a></p>
<p>It is maintained by Google and we recently added support for QKeras model to hls4ml.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.regularizers</span> <span class="kn">import</span> <span class="n">l1</span>
<span class="kn">from</span> <span class="nn">callbacks</span> <span class="kn">import</span> <span class="n">all_callbacks</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="nn">qkeras.qlayers</span> <span class="kn">import</span> <span class="n">QDense</span><span class="p">,</span> <span class="n">QActivation</span>
<span class="kn">from</span> <span class="nn">qkeras.quantizers</span> <span class="kn">import</span> <span class="n">quantized_bits</span><span class="p">,</span> <span class="n">quantized_relu</span>
</pre></div>
</div>
</div>
</div>
<p>We’re using <code class="docutils literal notranslate"><span class="pre">QDense</span></code> layer instead of <code class="docutils literal notranslate"><span class="pre">Dense</span></code>, and <code class="docutils literal notranslate"><span class="pre">QActivation</span></code> instead of <code class="docutils literal notranslate"><span class="pre">Activation</span></code>. We’re also specifying <code class="docutils literal notranslate"><span class="pre">kernel_quantizer</span> <span class="pre">=</span> <span class="pre">quantized_bits(6,0,0)</span></code>. This will use 6-bits (of which 0 are integer) for the weights. We also use the same quantization for the biases, and <code class="docutils literal notranslate"><span class="pre">quantized_relu(6)</span></code> for 6-bit ReLU activations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">QDense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc1&#39;</span><span class="p">,</span>
                 <span class="n">kernel_quantizer</span><span class="o">=</span><span class="n">quantized_bits</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">bias_quantizer</span><span class="o">=</span><span class="n">quantized_bits</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">QActivation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">quantized_relu</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">QDense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc2&#39;</span><span class="p">,</span>
                 <span class="n">kernel_quantizer</span><span class="o">=</span><span class="n">quantized_bits</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">bias_quantizer</span><span class="o">=</span><span class="n">quantized_bits</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">QActivation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">quantized_relu</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu2&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">QDense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc3&#39;</span><span class="p">,</span>
                 <span class="n">kernel_quantizer</span><span class="o">=</span><span class="n">quantized_bits</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">bias_quantizer</span><span class="o">=</span><span class="n">quantized_bits</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">QActivation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">quantized_relu</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu3&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">QDense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">,</span>
                 <span class="n">kernel_quantizer</span><span class="o">=</span><span class="n">quantized_bits</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">bias_quantizer</span><span class="o">=</span><span class="n">quantized_bits</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-07-26 21:04:31.560987: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-sparse">
<h2>Train sparse<a class="headerlink" href="#train-sparse" title="Permalink to this headline">#</a></h2>
<p>Let’s train with model sparsity again, since QKeras layers are prunable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow_model_optimization.python.core.sparsity.keras</span> <span class="kn">import</span> <span class="n">prune</span><span class="p">,</span> <span class="n">pruning_callbacks</span><span class="p">,</span> <span class="n">pruning_schedule</span>
<span class="kn">from</span> <span class="nn">tensorflow_model_optimization.sparsity.keras</span> <span class="kn">import</span> <span class="n">strip_pruning</span>
<span class="n">pruning_params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;pruning_schedule&quot;</span> <span class="p">:</span> <span class="n">pruning_schedule</span><span class="o">.</span><span class="n">ConstantSparsity</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">begin_step</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="mi">100</span><span class="p">)}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prune</span><span class="o">.</span><span class="n">prune_low_magnitude</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">pruning_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:212: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  mask = self.add_variable(
/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:219: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  threshold = self.add_variable(
/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:233: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  self.pruning_step = self.add_variable(
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-the-model">
<h2>Train the model<a class="headerlink" href="#train-the-model" title="Permalink to this headline">#</a></h2>
<p>We’ll use the same settings as the model for part 1: Adam optimizer with categorical crossentropy loss.
The callbacks will decay the learning rate and save the model into a directory ‘model_2’
The model isn’t very complex, so this should just take a few minutes even on the CPU.
If you’ve restarted the notebook kernel after training once, set <code class="docutils literal notranslate"><span class="pre">train</span> <span class="pre">=</span> <span class="pre">False</span></code> to load the trained model rather than training again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">train</span><span class="p">:</span>
    <span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">],</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">all_callbacks</span><span class="p">(</span><span class="n">stop_patience</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                              <span class="n">lr_factor</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                              <span class="n">lr_patience</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                              <span class="n">lr_epsilon</span> <span class="o">=</span> <span class="mf">0.000001</span><span class="p">,</span>
                              <span class="n">lr_cooldown</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                              <span class="n">lr_minimum</span> <span class="o">=</span> <span class="mf">0.0000001</span><span class="p">,</span>
                              <span class="n">outputDir</span> <span class="o">=</span> <span class="s1">&#39;model_4&#39;</span><span class="p">)</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pruning_callbacks</span><span class="o">.</span><span class="n">UpdatePruningStep</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_val</span><span class="p">,</span> <span class="n">y_train_val</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
              <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">callbacks</span> <span class="o">=</span> <span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="p">)</span>
    <span class="c1"># Save the model again but with the pruning &#39;stripped&#39; to use the regular layer types</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">strip_pruning</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;model_4/KERAS_check_best_model.h5&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
    <span class="kn">from</span> <span class="nn">qkeras.utils</span> <span class="kn">import</span> <span class="n">_add_supported_quantized_objects</span>
    <span class="n">co</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">_add_supported_quantized_objects</span><span class="p">(</span><span class="n">co</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;model_4/KERAS_check_best_model.h5&#39;</span><span class="p">,</span> <span class="n">custom_objects</span><span class="o">=</span><span class="n">co</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
Epoch 1/30
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  1/487 [..............................] - ETA: 21:32 - loss: 1.6368 - accuracy: 0.3262WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0049s vs `on_train_batch_end` time: 0.0068s). Check your callbacks.
480/487 [============================&gt;.] - ETA: 0s - loss: 1.3881 - accuracy: 0.4900
***callbacks***
saving losses to model_4/losses.log

Epoch 1: val_loss improved from inf to 1.19395, saving model to model_4/KERAS_check_best_model.h5

Epoch 1: val_loss improved from inf to 1.19395, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 1: saving model to model_4/KERAS_check_model_last.h5

Epoch 1: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 5s 5ms/step - loss: 1.3854 - accuracy: 0.4916 - val_loss: 1.1940 - val_accuracy: 0.6129 - lr: 1.0000e-04
Epoch 2/30
479/487 [============================&gt;.] - ETA: 0s - loss: 1.1089 - accuracy: 0.6524
***callbacks***
saving losses to model_4/losses.log

Epoch 2: val_loss improved from 1.19395 to 1.04657, saving model to model_4/KERAS_check_best_model.h5

Epoch 2: val_loss improved from 1.19395 to 1.04657, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 2: saving model to model_4/KERAS_check_model_last.h5

Epoch 2: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 1.1079 - accuracy: 0.6529 - val_loss: 1.0466 - val_accuracy: 0.6832 - lr: 1.0000e-04
Epoch 3/30
475/487 [============================&gt;.] - ETA: 0s - loss: 1.0017 - accuracy: 0.7012
***callbacks***
saving losses to model_4/losses.log

Epoch 3: val_loss improved from 1.04657 to 0.96324, saving model to model_4/KERAS_check_best_model.h5

Epoch 3: val_loss improved from 1.04657 to 0.96324, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 3: saving model to model_4/KERAS_check_model_last.h5

Epoch 3: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 1.0008 - accuracy: 0.7014 - val_loss: 0.9632 - val_accuracy: 0.7125 - lr: 1.0000e-04
Epoch 4/30
477/487 [============================&gt;.] - ETA: 0s - loss: 0.9312 - accuracy: 0.7176
***callbacks***
saving losses to model_4/losses.log

Epoch 4: val_loss improved from 0.96324 to 0.90714, saving model to model_4/KERAS_check_best_model.h5

Epoch 4: val_loss improved from 0.96324 to 0.90714, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 4: saving model to model_4/KERAS_check_model_last.h5

Epoch 4: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.9308 - accuracy: 0.7175 - val_loss: 0.9071 - val_accuracy: 0.7217 - lr: 1.0000e-04
Epoch 5/30
479/487 [============================&gt;.] - ETA: 0s - loss: 1.1106 - accuracy: 0.6638
***callbacks***
saving losses to model_4/losses.log

Epoch 5: val_loss did not improve from 0.90714

Epoch 5: val_loss did not improve from 0.90714

Epoch 5: saving model to model_4/KERAS_check_model_last.h5

Epoch 5: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 1.1099 - accuracy: 0.6643 - val_loss: 1.0583 - val_accuracy: 0.6908 - lr: 1.0000e-04
Epoch 6/30
476/487 [============================&gt;.] - ETA: 0s - loss: 1.0147 - accuracy: 0.7012
***callbacks***
saving losses to model_4/losses.log

Epoch 6: val_loss did not improve from 0.90714

Epoch 6: val_loss did not improve from 0.90714

Epoch 6: saving model to model_4/KERAS_check_model_last.h5

Epoch 6: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 1.0142 - accuracy: 0.7013 - val_loss: 0.9864 - val_accuracy: 0.7071 - lr: 1.0000e-04
Epoch 7/30
476/487 [============================&gt;.] - ETA: 0s - loss: 0.9588 - accuracy: 0.7113
***callbacks***
saving losses to model_4/losses.log

Epoch 7: val_loss did not improve from 0.90714

Epoch 7: val_loss did not improve from 0.90714

Epoch 7: saving model to model_4/KERAS_check_model_last.h5

Epoch 7: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.9583 - accuracy: 0.7115 - val_loss: 0.9409 - val_accuracy: 0.7147 - lr: 1.0000e-04
Epoch 8/30
472/487 [============================&gt;.] - ETA: 0s - loss: 0.9231 - accuracy: 0.7164
***callbacks***
saving losses to model_4/losses.log

Epoch 8: val_loss did not improve from 0.90714

Epoch 8: val_loss did not improve from 0.90714

Epoch 8: saving model to model_4/KERAS_check_model_last.h5

Epoch 8: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.9223 - accuracy: 0.7168 - val_loss: 0.9106 - val_accuracy: 0.7185 - lr: 1.0000e-04
Epoch 9/30
477/487 [============================&gt;.] - ETA: 0s - loss: 0.8967 - accuracy: 0.7202
***callbacks***
saving losses to model_4/losses.log

Epoch 9: val_loss improved from 0.90714 to 0.88768, saving model to model_4/KERAS_check_best_model.h5

Epoch 9: val_loss improved from 0.90714 to 0.88768, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 9: saving model to model_4/KERAS_check_model_last.h5

Epoch 9: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8963 - accuracy: 0.7203 - val_loss: 0.8877 - val_accuracy: 0.7214 - lr: 1.0000e-04
Epoch 10/30
479/487 [============================&gt;.] - ETA: 0s - loss: 0.8749 - accuracy: 0.7233
***callbacks***
saving losses to model_4/losses.log

Epoch 10: val_loss improved from 0.88768 to 0.86825, saving model to model_4/KERAS_check_best_model.h5

Epoch 10: val_loss improved from 0.88768 to 0.86825, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 10: saving model to model_4/KERAS_check_model_last.h5

Epoch 10: saving model to model_4/KERAS_check_model_last_weights.h5

Epoch 10: saving model to model_4/KERAS_check_model_epoch10.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8749 - accuracy: 0.7233 - val_loss: 0.8682 - val_accuracy: 0.7241 - lr: 1.0000e-04
Epoch 11/30
475/487 [============================&gt;.] - ETA: 0s - loss: 0.8570 - accuracy: 0.7259
***callbacks***
saving losses to model_4/losses.log

Epoch 11: val_loss improved from 0.86825 to 0.85209, saving model to model_4/KERAS_check_best_model.h5

Epoch 11: val_loss improved from 0.86825 to 0.85209, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 11: saving model to model_4/KERAS_check_model_last.h5

Epoch 11: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8568 - accuracy: 0.7260 - val_loss: 0.8521 - val_accuracy: 0.7262 - lr: 1.0000e-04
Epoch 12/30
479/487 [============================&gt;.] - ETA: 0s - loss: 0.8409 - accuracy: 0.7285
***callbacks***
saving losses to model_4/losses.log

Epoch 12: val_loss improved from 0.85209 to 0.83657, saving model to model_4/KERAS_check_best_model.h5

Epoch 12: val_loss improved from 0.85209 to 0.83657, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 12: saving model to model_4/KERAS_check_model_last.h5

Epoch 12: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8409 - accuracy: 0.7285 - val_loss: 0.8366 - val_accuracy: 0.7288 - lr: 1.0000e-04
Epoch 13/30
484/487 [============================&gt;.] - ETA: 0s - loss: 0.8276 - accuracy: 0.7308
***callbacks***
saving losses to model_4/losses.log

Epoch 13: val_loss improved from 0.83657 to 0.82468, saving model to model_4/KERAS_check_best_model.h5

Epoch 13: val_loss improved from 0.83657 to 0.82468, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 13: saving model to model_4/KERAS_check_model_last.h5
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 13: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8275 - accuracy: 0.7308 - val_loss: 0.8247 - val_accuracy: 0.7308 - lr: 1.0000e-04
Epoch 14/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.8162 - accuracy: 0.7326
***callbacks***
saving losses to model_4/losses.log

Epoch 14: val_loss improved from 0.82468 to 0.81405, saving model to model_4/KERAS_check_best_model.h5

Epoch 14: val_loss improved from 0.82468 to 0.81405, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 14: saving model to model_4/KERAS_check_model_last.h5

Epoch 14: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8160 - accuracy: 0.7326 - val_loss: 0.8141 - val_accuracy: 0.7329 - lr: 1.0000e-04
Epoch 15/30
487/487 [==============================] - ETA: 0s - loss: 0.8063 - accuracy: 0.7345
***callbacks***
saving losses to model_4/losses.log

Epoch 15: val_loss improved from 0.81405 to 0.80517, saving model to model_4/KERAS_check_best_model.h5

Epoch 15: val_loss improved from 0.81405 to 0.80517, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 15: saving model to model_4/KERAS_check_model_last.h5

Epoch 15: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8063 - accuracy: 0.7345 - val_loss: 0.8052 - val_accuracy: 0.7341 - lr: 1.0000e-04
Epoch 16/30
472/487 [============================&gt;.] - ETA: 0s - loss: 0.7979 - accuracy: 0.7361
***callbacks***
saving losses to model_4/losses.log

Epoch 16: val_loss improved from 0.80517 to 0.79793, saving model to model_4/KERAS_check_best_model.h5

Epoch 16: val_loss improved from 0.80517 to 0.79793, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 16: saving model to model_4/KERAS_check_model_last.h5

Epoch 16: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7979 - accuracy: 0.7361 - val_loss: 0.7979 - val_accuracy: 0.7363 - lr: 1.0000e-04
Epoch 17/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.7907 - accuracy: 0.7377
***callbacks***
saving losses to model_4/losses.log

Epoch 17: val_loss improved from 0.79793 to 0.79151, saving model to model_4/KERAS_check_best_model.h5

Epoch 17: val_loss improved from 0.79793 to 0.79151, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 17: saving model to model_4/KERAS_check_model_last.h5

Epoch 17: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7907 - accuracy: 0.7377 - val_loss: 0.7915 - val_accuracy: 0.7379 - lr: 1.0000e-04
Epoch 18/30
487/487 [==============================] - ETA: 0s - loss: 0.7845 - accuracy: 0.7391
***callbacks***
saving losses to model_4/losses.log

Epoch 18: val_loss improved from 0.79151 to 0.78593, saving model to model_4/KERAS_check_best_model.h5

Epoch 18: val_loss improved from 0.79151 to 0.78593, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 18: saving model to model_4/KERAS_check_model_last.h5

Epoch 18: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7845 - accuracy: 0.7391 - val_loss: 0.7859 - val_accuracy: 0.7384 - lr: 1.0000e-04
Epoch 19/30
472/487 [============================&gt;.] - ETA: 0s - loss: 0.7796 - accuracy: 0.7406
***callbacks***
saving losses to model_4/losses.log

Epoch 19: val_loss improved from 0.78593 to 0.78121, saving model to model_4/KERAS_check_best_model.h5

Epoch 19: val_loss improved from 0.78593 to 0.78121, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 19: saving model to model_4/KERAS_check_model_last.h5

Epoch 19: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7793 - accuracy: 0.7406 - val_loss: 0.7812 - val_accuracy: 0.7401 - lr: 1.0000e-04
Epoch 20/30
487/487 [==============================] - ETA: 0s - loss: 0.7750 - accuracy: 0.7414
***callbacks***
saving losses to model_4/losses.log

Epoch 20: val_loss improved from 0.78121 to 0.77640, saving model to model_4/KERAS_check_best_model.h5

Epoch 20: val_loss improved from 0.78121 to 0.77640, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 20: saving model to model_4/KERAS_check_model_last.h5

Epoch 20: saving model to model_4/KERAS_check_model_last_weights.h5

Epoch 20: saving model to model_4/KERAS_check_model_epoch20.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7750 - accuracy: 0.7414 - val_loss: 0.7764 - val_accuracy: 0.7411 - lr: 1.0000e-04
Epoch 21/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.7707 - accuracy: 0.7427
***callbacks***
saving losses to model_4/losses.log

Epoch 21: val_loss improved from 0.77640 to 0.77285, saving model to model_4/KERAS_check_best_model.h5

Epoch 21: val_loss improved from 0.77640 to 0.77285, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 21: saving model to model_4/KERAS_check_model_last.h5

Epoch 21: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7705 - accuracy: 0.7427 - val_loss: 0.7728 - val_accuracy: 0.7418 - lr: 1.0000e-04
Epoch 22/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.7672 - accuracy: 0.7433
***callbacks***
saving losses to model_4/losses.log

Epoch 22: val_loss improved from 0.77285 to 0.76974, saving model to model_4/KERAS_check_best_model.h5

Epoch 22: val_loss improved from 0.77285 to 0.76974, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 22: saving model to model_4/KERAS_check_model_last.h5

Epoch 22: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7672 - accuracy: 0.7433 - val_loss: 0.7697 - val_accuracy: 0.7425 - lr: 1.0000e-04
Epoch 23/30
474/487 [============================&gt;.] - ETA: 0s - loss: 0.7645 - accuracy: 0.7442
***callbacks***
saving losses to model_4/losses.log

Epoch 23: val_loss improved from 0.76974 to 0.76717, saving model to model_4/KERAS_check_best_model.h5

Epoch 23: val_loss improved from 0.76974 to 0.76717, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 23: saving model to model_4/KERAS_check_model_last.h5

Epoch 23: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 5ms/step - loss: 0.7642 - accuracy: 0.7443 - val_loss: 0.7672 - val_accuracy: 0.7432 - lr: 1.0000e-04
Epoch 24/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.7618 - accuracy: 0.7449
***callbacks***
saving losses to model_4/losses.log

Epoch 24: val_loss improved from 0.76717 to 0.76459, saving model to model_4/KERAS_check_best_model.h5

Epoch 24: val_loss improved from 0.76717 to 0.76459, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 24: saving model to model_4/KERAS_check_model_last.h5

Epoch 24: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7618 - accuracy: 0.7449 - val_loss: 0.7646 - val_accuracy: 0.7437 - lr: 1.0000e-04
Epoch 25/30
485/487 [============================&gt;.] - ETA: 0s - loss: 0.7591 - accuracy: 0.7456
***callbacks***
saving losses to model_4/losses.log

Epoch 25: val_loss improved from 0.76459 to 0.76249, saving model to model_4/KERAS_check_best_model.h5

Epoch 25: val_loss improved from 0.76459 to 0.76249, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 25: saving model to model_4/KERAS_check_model_last.h5

Epoch 25: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 5ms/step - loss: 0.7590 - accuracy: 0.7456 - val_loss: 0.7625 - val_accuracy: 0.7444 - lr: 1.0000e-04
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 26/30
484/487 [============================&gt;.] - ETA: 0s - loss: 0.7570 - accuracy: 0.7462
***callbacks***
saving losses to model_4/losses.log

Epoch 26: val_loss improved from 0.76249 to 0.76052, saving model to model_4/KERAS_check_best_model.h5

Epoch 26: val_loss improved from 0.76249 to 0.76052, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 26: saving model to model_4/KERAS_check_model_last.h5

Epoch 26: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7571 - accuracy: 0.7462 - val_loss: 0.7605 - val_accuracy: 0.7450 - lr: 1.0000e-04
Epoch 27/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.7554 - accuracy: 0.7467
***callbacks***
saving losses to model_4/losses.log

Epoch 27: val_loss improved from 0.76052 to 0.75954, saving model to model_4/KERAS_check_best_model.h5

Epoch 27: val_loss improved from 0.76052 to 0.75954, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 27: saving model to model_4/KERAS_check_model_last.h5

Epoch 27: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7554 - accuracy: 0.7467 - val_loss: 0.7595 - val_accuracy: 0.7454 - lr: 1.0000e-04
Epoch 28/30
479/487 [============================&gt;.] - ETA: 0s - loss: 0.7535 - accuracy: 0.7473
***callbacks***
saving losses to model_4/losses.log

Epoch 28: val_loss improved from 0.75954 to 0.75774, saving model to model_4/KERAS_check_best_model.h5

Epoch 28: val_loss improved from 0.75954 to 0.75774, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 28: saving model to model_4/KERAS_check_model_last.h5

Epoch 28: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7536 - accuracy: 0.7472 - val_loss: 0.7577 - val_accuracy: 0.7458 - lr: 1.0000e-04
Epoch 29/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.7522 - accuracy: 0.7477
***callbacks***
saving losses to model_4/losses.log

Epoch 29: val_loss improved from 0.75774 to 0.75659, saving model to model_4/KERAS_check_best_model.h5

Epoch 29: val_loss improved from 0.75774 to 0.75659, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 29: saving model to model_4/KERAS_check_model_last.h5

Epoch 29: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7522 - accuracy: 0.7476 - val_loss: 0.7566 - val_accuracy: 0.7461 - lr: 1.0000e-04
Epoch 30/30
479/487 [============================&gt;.] - ETA: 0s - loss: 0.7508 - accuracy: 0.7479
***callbacks***
saving losses to model_4/losses.log

Epoch 30: val_loss improved from 0.75659 to 0.75489, saving model to model_4/KERAS_check_best_model.h5

Epoch 30: val_loss improved from 0.75659 to 0.75489, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 30: saving model to model_4/KERAS_check_model_last.h5

Epoch 30: saving model to model_4/KERAS_check_model_last_weights.h5

Epoch 30: saving model to model_4/KERAS_check_model_epoch30.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7505 - accuracy: 0.7481 - val_loss: 0.7549 - val_accuracy: 0.7468 - lr: 1.0000e-04
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
</pre></div>
</div>
</div>
</div>
</section>
<section id="check-performance">
<h2>Check performance<a class="headerlink" href="#check-performance" title="Permalink to this headline">#</a></h2>
<p>How does this model which was trained using 6-bits, and 75% sparsity model compare against the original model? Let’s report the accuracy and make a ROC curve. The quantized, pruned model is shown with solid lines, the unpruned model from part 1 is shown with dashed lines.</p>
<p>We should also check that hls4ml can respect the choice to use 6-bits throughout the model, and match the accuracy. We’ll generate a configuration from this Quantized model, and plot its performance as the dotted line.
The generated configuration is printed out. You’ll notice that it uses 7 bits for the type, but we specified 6!? That’s just because QKeras doesn’t count the sign-bit when we specify the number of bits, so the type that actually gets used needs 1 more.</p>
<p>We also use the <code class="docutils literal notranslate"><span class="pre">OutputRoundingSaturationMode</span></code> optimizer pass of <code class="docutils literal notranslate"><span class="pre">hls4ml</span></code> to set the Activation layers to round, rather than truncate, the cast. This is important for getting good model accuracy when using small bit precision activations. And we’ll set a different data type for the tables used in the Softmax, just for a bit of extra performance.</p>
<p><strong>Make sure you’ve trained the model from part 1</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">hls4ml</span>
<span class="kn">import</span> <span class="nn">plotting</span>
<span class="n">hls4ml</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">optimizer_pass</span><span class="p">(</span><span class="s1">&#39;output_rounding_saturation_mode&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Activation&#39;</span><span class="p">]</span>
<span class="n">hls4ml</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">optimizer_pass</span><span class="p">(</span><span class="s1">&#39;output_rounding_saturation_mode&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rounding_mode</span> <span class="o">=</span> <span class="s1">&#39;AP_RND&#39;</span>
<span class="n">hls4ml</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">optimizer_pass</span><span class="p">(</span><span class="s1">&#39;output_rounding_saturation_mode&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">saturation_mode</span> <span class="o">=</span> <span class="s1">&#39;AP_SAT&#39;</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">hls4ml</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">config_from_keras_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">granularity</span><span class="o">=</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
<span class="n">config</span><span class="p">[</span><span class="s1">&#39;LayerName&#39;</span><span class="p">][</span><span class="s1">&#39;softmax&#39;</span><span class="p">][</span><span class="s1">&#39;exp_table_t&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;ap_fixed&lt;18,8&gt;&#39;</span>
<span class="n">config</span><span class="p">[</span><span class="s1">&#39;LayerName&#39;</span><span class="p">][</span><span class="s1">&#39;softmax&#39;</span><span class="p">][</span><span class="s1">&#39;inv_table_t&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;ap_fixed&lt;18,4&gt;&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-----------------------------------&quot;</span><span class="p">)</span>
<span class="n">plotting</span><span class="o">.</span><span class="n">print_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-----------------------------------&quot;</span><span class="p">)</span>
<span class="n">hls_model</span> <span class="o">=</span> <span class="n">hls4ml</span><span class="o">.</span><span class="n">converters</span><span class="o">.</span><span class="n">convert_from_keras_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                                       <span class="n">hls_config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
                                                       <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;model_4/hls4ml_prj&#39;</span><span class="p">,</span>
                                                       <span class="n">part</span><span class="o">=</span><span class="s1">&#39;xcu250-figd2104-2L-e&#39;</span><span class="p">)</span>
<span class="n">hls_model</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

<span class="n">y_qkeras</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="n">y_hls</span> <span class="o">=</span> <span class="n">hls_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Interpreting Sequential
Topology:
Layer name: fc1_input, layer type: Input
Layer name: fc1, layer type: QDense
  -&gt; Activation (linear), layer name: fc1
Layer name: relu1, layer type: QActivation
  -&gt; Activation (quantized_relu), layer name: relu1
Layer name: fc2, layer type: QDense
  -&gt; Activation (linear), layer name: fc2
Layer name: relu2, layer type: QActivation
  -&gt; Activation (quantized_relu), layer name: relu2
Layer name: fc3, layer type: QDense
  -&gt; Activation (linear), layer name: fc3
Layer name: relu3, layer type: QActivation
  -&gt; Activation (quantized_relu), layer name: relu3
Layer name: output, layer type: QDense
  -&gt; Activation (linear), layer name: output
Layer name: softmax, layer type: Activation
-----------------------------------
Model
  Precision:         ap_fixed&lt;16,6&gt;
  ReuseFactor:       1
  Strategy:          Latency
LayerName
  fc1_input
    Precision
      result:        ap_fixed&lt;16,6&gt;
  fc1
    Precision
      weight:        ap_fixed&lt;6,1&gt;
      bias:          ap_fixed&lt;6,1&gt;
    ReuseFactor:     1
  fc1_linear
    Precision:       ap_fixed&lt;16,6&gt;
    ReuseFactor:     1
    table_size:      1024
    table_t:         ap_fixed&lt;18,8&gt;
  relu1
    Precision
      result:        ap_ufixed&lt;6,0&gt;
    ReuseFactor:     1
  relu1_quantized_relu
    Precision
      result:        ap_ufixed&lt;6,0&gt;
    ReuseFactor:     1
  fc2
    Precision
      weight:        ap_fixed&lt;6,1&gt;
      bias:          ap_fixed&lt;6,1&gt;
    ReuseFactor:     1
  fc2_linear
    Precision:       ap_fixed&lt;16,6&gt;
    ReuseFactor:     1
    table_size:      1024
    table_t:         ap_fixed&lt;18,8&gt;
  relu2
    Precision
      result:        ap_ufixed&lt;6,0&gt;
    ReuseFactor:     1
  relu2_quantized_relu
    Precision
      result:        ap_ufixed&lt;6,0&gt;
    ReuseFactor:     1
  fc3
    Precision
      weight:        ap_fixed&lt;6,1&gt;
      bias:          ap_fixed&lt;6,1&gt;
    ReuseFactor:     1
  fc3_linear
    Precision:       ap_fixed&lt;16,6&gt;
    ReuseFactor:     1
    table_size:      1024
    table_t:         ap_fixed&lt;18,8&gt;
  relu3
    Precision
      result:        ap_ufixed&lt;6,0&gt;
    ReuseFactor:     1
  relu3_quantized_relu
    Precision
      result:        ap_ufixed&lt;6,0&gt;
    ReuseFactor:     1
  output
    Precision
      weight:        ap_fixed&lt;6,1&gt;
      bias:          ap_fixed&lt;6,1&gt;
    ReuseFactor:     1
  output_linear
    Precision:       ap_fixed&lt;16,6&gt;
    ReuseFactor:     1
    table_size:      1024
    table_t:         ap_fixed&lt;18,8&gt;
  softmax
    Precision:       ap_fixed&lt;16,6&gt;
    ReuseFactor:     1
    table_size:      1024
    exp_table_t:     ap_fixed&lt;18,8&gt;
    inv_table_t:     ap_fixed&lt;18,4&gt;
-----------------------------------
Interpreting Sequential
Topology:
Layer name: fc1_input, layer type: InputLayer, input shapes: [[None, 16]], output shape: [None, 16]
Layer name: fc1, layer type: QDense, input shapes: [[None, 16]], output shape: [None, 64]
Layer name: relu1, layer type: Activation, input shapes: [[None, 64]], output shape: [None, 64]
Layer name: fc2, layer type: QDense, input shapes: [[None, 64]], output shape: [None, 32]
Layer name: relu2, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]
Layer name: fc3, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 32]
Layer name: relu3, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]
Layer name: output, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 5]
Layer name: softmax, layer type: Softmax, input shapes: [[None, 5]], output shape: [None, 5]
Creating HLS model
Writing HLS project
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
Done
5188/5188 [==============================] - 4s 782us/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

<span class="n">model_ref</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;model_1/KERAS_check_best_model.h5&#39;</span><span class="p">)</span>
<span class="n">y_ref</span> <span class="o">=</span> <span class="n">model_ref</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy baseline:  </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_ref</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy pruned, quantized: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_qkeras</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy hls4ml: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_hls</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_ref</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_qkeras</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hls</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>
<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">),</span>
         <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">),</span>
         <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)]</span>
<span class="kn">from</span> <span class="nn">matplotlib.legend</span> <span class="kn">import</span> <span class="n">Legend</span>
<span class="n">leg</span> <span class="o">=</span> <span class="n">Legend</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">lines</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;baseline&#39;</span><span class="p">,</span> <span class="s1">&#39;pruned, quantized&#39;</span><span class="p">,</span> <span class="s1">&#39;hls4ml&#39;</span><span class="p">],</span>
            <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">leg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5188/5188 [==============================] - 4s 704us/step
Accuracy baseline:  0.7502650602409638
Accuracy pruned, quantized: 0.7454819277108434
Accuracy hls4ml: 0.5828132530120482
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x15656ddf0&gt;
</pre></div>
</div>
<img alt="_images/2.4_quantization_14_2.png" src="_images/2.4_quantization_14_2.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="synthesize">
<h1>Synthesize<a class="headerlink" href="#synthesize" title="Permalink to this headline">#</a></h1>
<p>Now let’s synthesize this quantized, pruned model.</p>
<p><strong>The synthesis will take a while</strong></p>
<p>While the C-Synthesis is running, we can monitor the progress looking at the log file by opening a terminal from the notebook home, and executing:</p>
<p><code class="docutils literal notranslate"><span class="pre">tail</span> <span class="pre">-f</span> <span class="pre">model_3/hls4ml_prj/vivado_hls.log</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hls_model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">csim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="check-the-reports">
<h2>Check the reports<a class="headerlink" href="#check-the-reports" title="Permalink to this headline">#</a></h2>
<p>Print out the reports generated by Vivado HLS. Pay attention to the Utilization Estimates’ section in particular this time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hls4ml</span><span class="o">.</span><span class="n">report</span><span class="o">.</span><span class="n">read_vivado_report</span><span class="p">(</span><span class="s1">&#39;model_3/hls4ml_prj&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Print the report for the model trained in part 1. Now, compared to the model from part 1, this model has been trained with low-precision quantization, and 75% pruning. You should be able to see that we have saved a lot of resource compared to where we started in part 1. At the same time, referring to the ROC curve above, the model performance is pretty much identical even with this drastic compression!</p>
<p><strong>Note you need to have trained and synthesized the model from part 1</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hls4ml</span><span class="o">.</span><span class="n">report</span><span class="o">.</span><span class="n">read_vivado_report</span><span class="p">(</span><span class="s1">&#39;model_1/hls4ml_prj&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Print the report for the model trained in part 3. Both these models were trained with 75% sparsity, but the new model uses 6-bit precision as well. You can see how Vivado HLS has moved multiplication operations from DSPs into LUTs, reducing the “critical” resource usage.</p>
<p><strong>Note you need to have trained and synthesized the model from part 3</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hls4ml</span><span class="o">.</span><span class="n">report</span><span class="o">.</span><span class="n">read_vivado_report</span><span class="p">(</span><span class="s1">&#39;model_2/hls4ml_prj&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="nb">
<h2>NB<a class="headerlink" href="#nb" title="Permalink to this headline">#</a></h2>
<p>Note as well that the Vivado HLS resource estimates tend to <em>overestimate</em> LUTs, while generally estimating the DSPs correctly. Running the subsequent stages of FPGA compilation reveals the more realistic resource usage, You can run the next step, ‘logic synthesis’ with <code class="docutils literal notranslate"><span class="pre">hls_model.build(synth=True,</span> <span class="pre">vsynth=True)</span></code>, but we skipped it in this tutorial in the interest of time.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="2.3_compression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Part 3: Compression</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book Community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>