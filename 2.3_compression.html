
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Model Compression &#8212; IAIFI Summer School Tutorials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://jduarte.physics.ucsd.edu/iaifi-summer-school/2.3_compression.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model Quantization" href="2.4_quantization.html" />
    <link rel="prev" title="Advanced hls4ml Configuration" href="2.2_advanced_config.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">IAIFI Summer School Tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    IAIFI Summer School Tutorials
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Representations, Networks, and Symmetries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1.1_tabular_data_efps.html">
   Tabular Data using Energy Flow Polynomials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.3_deep_sets.html">
   Deep Sets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.4_gnn_in.html">
   Interaction Network GNN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.5_gnn_lorentz.html">
   Lorentz-Equivariant GNN
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model Compression and Fast Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="2.1_getting_started.html">
   Getting Started with hls4ml
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2.2_advanced_config.html">
   Advanced hls4ml Configuration
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Model Compression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2.4_quantization.html">
   Model Quantization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/jmduarte/iaifi-summer-school/main?urlpath=tree/book/2.3_compression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/jmduarte/iaifi-summer-school/blob/main/book/2.3_compression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jmduarte/iaifi-summer-school"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jmduarte/iaifi-summer-school/issues/new?title=Issue%20on%20page%20%2F2.3_compression.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/2.3_compression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fetch-the-jet-tagging-dataset-from-open-ml">
   Fetch the jet tagging dataset from Open ML
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#now-construct-a-model">
   Now construct a model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-sparse">
   Train sparse
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-the-model">
   Train the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-sparsity">
   Check sparsity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-performance">
   Check performance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Train the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Check sparsity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   Check performance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convert-the-model-to-fpga-firmware-with-hls4ml">
   Convert the model to FPGA firmware with hls4ml
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-the-reports">
   Check the reports
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Model Compression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fetch-the-jet-tagging-dataset-from-open-ml">
   Fetch the jet tagging dataset from Open ML
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#now-construct-a-model">
   Now construct a model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-sparse">
   Train sparse
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-the-model">
   Train the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-sparsity">
   Check sparsity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-performance">
   Check performance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Train the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Check sparsity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   Check performance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convert-the-model-to-fpga-firmware-with-hls4ml">
   Convert the model to FPGA firmware with hls4ml
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-the-reports">
   Check the reports
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="model-compression">
<h1>Model Compression<a class="headerlink" href="#model-compression" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="c1">#import os</span>
<span class="c1">#os.environ[&#39;PATH&#39;] = &#39;/opt/Xilinx/Vivado/2019.2/bin:&#39; + os.environ[&#39;PATH&#39;]</span>
<span class="c1"># for this tutorial we wont be actually running Vivado, so I have commented these lines out</span>
<span class="c1">#     but if you want to look into actually running on an FPGA then simply uncomment these lines</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-07-31 18:28:49.986039: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.7.13/x64/lib
2022-07-31 18:28:49.986080: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
</pre></div>
</div>
</div>
</div>
<section id="fetch-the-jet-tagging-dataset-from-open-ml">
<h2>Fetch the jet tagging dataset from Open ML<a class="headerlink" href="#fetch-the-jet-tagging-dataset-from-open-ml" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;X_train_val.npy&#39;</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;X_test.npy&#39;</span><span class="p">)</span>
<span class="n">y_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;y_train_val.npy&#39;</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;y_test.npy&#39;</span><span class="p">)</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;classes.npy&#39;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_5088</span><span class="o">/</span><span class="mf">2878572050.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">X_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;X_train_val.npy&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;X_test.npy&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">y_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;y_train_val.npy&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;y_test.npy&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;classes.npy&#39;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/numpy/lib/npyio.py</span> in <span class="ni">load</span><span class="nt">(file, mmap_mode, allow_pickle, fix_imports, encoding)</span>
<span class="g g-Whitespace">    </span><span class="mi">415</span>             <span class="n">own_fid</span> <span class="o">=</span> <span class="kc">False</span>
<span class="g g-Whitespace">    </span><span class="mi">416</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">417</span>             <span class="n">fid</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">os_fspath</span><span class="p">(</span><span class="n">file</span><span class="p">),</span> <span class="s2">&quot;rb&quot;</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">418</span>             <span class="n">own_fid</span> <span class="o">=</span> <span class="kc">True</span>
<span class="g g-Whitespace">    </span><span class="mi">419</span> 

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;X_train_val.npy&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="now-construct-a-model">
<h2>Now construct a model<a class="headerlink" href="#now-construct-a-model" title="Permalink to this headline">#</a></h2>
<p>We’ll use the same architecture as in part 1: 3 hidden layers with 64, then 32, then 32 neurons. Each layer will use <code class="docutils literal notranslate"><span class="pre">relu</span></code> activation.
Add an output layer with 5 neurons (one for each class), then finish with Softmax activation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.regularizers</span> <span class="kn">import</span> <span class="n">l1</span>
<span class="kn">from</span> <span class="nn">callbacks</span> <span class="kn">import</span> <span class="n">all_callbacks</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc1&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu1&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc2&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu2&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc3&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;relu3&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;lecun_uniform&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-07-26 21:02:27.474108: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-sparse">
<h2>Train sparse<a class="headerlink" href="#train-sparse" title="Permalink to this headline">#</a></h2>
<p>This time we’ll use the Tensorflow model optimization sparsity to train a sparse model (forcing many weights to ‘0’). In this instance, the target sparsity is 75%</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow_model_optimization.python.core.sparsity.keras</span> <span class="kn">import</span> <span class="n">prune</span><span class="p">,</span> <span class="n">pruning_callbacks</span><span class="p">,</span> <span class="n">pruning_schedule</span>
<span class="kn">from</span> <span class="nn">tensorflow_model_optimization.sparsity.keras</span> <span class="kn">import</span> <span class="n">strip_pruning</span>
<span class="n">pruning_params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;pruning_schedule&quot;</span> <span class="p">:</span> <span class="n">pruning_schedule</span><span class="o">.</span><span class="n">ConstantSparsity</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">begin_step</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="mi">100</span><span class="p">)}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prune</span><span class="o">.</span><span class="n">prune_low_magnitude</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">pruning_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:212: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  mask = self.add_variable(
/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:219: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  threshold = self.add_variable(
/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:233: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  self.pruning_step = self.add_variable(
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-the-model">
<h2>Train the model<a class="headerlink" href="#train-the-model" title="Permalink to this headline">#</a></h2>
<p>We’ll use the same settings as the model for part 1: Adam optimizer with categorical crossentropy loss.
The callbacks will decay the learning rate and save the model into a directory ‘model_2’
The model isn’t very complex, so this should just take a few minutes even on the CPU.
If you’ve restarted the notebook kernel after training once, set <code class="docutils literal notranslate"><span class="pre">train</span> <span class="pre">=</span> <span class="pre">False</span></code> to load the trained model rather than training again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">train</span><span class="p">:</span>
    <span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">],</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">all_callbacks</span><span class="p">(</span><span class="n">stop_patience</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                              <span class="n">lr_factor</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                              <span class="n">lr_patience</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                              <span class="n">lr_epsilon</span> <span class="o">=</span> <span class="mf">0.000001</span><span class="p">,</span>
                              <span class="n">lr_cooldown</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                              <span class="n">lr_minimum</span> <span class="o">=</span> <span class="mf">0.0000001</span><span class="p">,</span>
                              <span class="n">outputDir</span> <span class="o">=</span> <span class="s1">&#39;model_2&#39;</span><span class="p">)</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pruning_callbacks</span><span class="o">.</span><span class="n">UpdatePruningStep</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_val</span><span class="p">,</span> <span class="n">y_train_val</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
              <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">callbacks</span> <span class="o">=</span> <span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="p">)</span>
    <span class="c1"># Save the model again but with the pruning &#39;stripped&#39; to use the regular layer types</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">strip_pruning</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;model_2/KERAS_check_best_model.h5&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;model_2/KERAS_check_best_model.h5&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
  1/487 [..............................] - ETA: 16:16 - loss: 1.6388 - accuracy: 0.3027WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0049s). Check your callbacks.
464/487 [===========================&gt;..] - ETA: 0s - loss: 1.3388 - accuracy: 0.5105
***callbacks***
saving losses to model_2/losses.log

Epoch 1: val_loss improved from inf to 1.12515, saving model to model_2/KERAS_check_best_model.h5

Epoch 1: val_loss improved from inf to 1.12515, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 1: saving model to model_2/KERAS_check_model_last.h5

Epoch 1: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 4s 3ms/step - loss: 1.3290 - accuracy: 0.5161 - val_loss: 1.1252 - val_accuracy: 0.6363 - lr: 1.0000e-04
Epoch 2/30
472/487 [============================&gt;.] - ETA: 0s - loss: 1.0568 - accuracy: 0.6661
***callbacks***
saving losses to model_2/losses.log

Epoch 2: val_loss improved from 1.12515 to 1.00706, saving model to model_2/KERAS_check_best_model.h5

Epoch 2: val_loss improved from 1.12515 to 1.00706, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 2: saving model to model_2/KERAS_check_model_last.h5

Epoch 2: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 1.0550 - accuracy: 0.6668 - val_loss: 1.0071 - val_accuracy: 0.6900 - lr: 1.0000e-04
Epoch 3/30
465/487 [===========================&gt;..] - ETA: 0s - loss: 0.9726 - accuracy: 0.7000
***callbacks***
saving losses to model_2/losses.log

Epoch 3: val_loss improved from 1.00706 to 0.94499, saving model to model_2/KERAS_check_best_model.h5

Epoch 3: val_loss improved from 1.00706 to 0.94499, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 3: saving model to model_2/KERAS_check_model_last.h5

Epoch 3: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9712 - accuracy: 0.7004 - val_loss: 0.9450 - val_accuracy: 0.7098 - lr: 1.0000e-04
Epoch 4/30
465/487 [===========================&gt;..] - ETA: 0s - loss: 0.9188 - accuracy: 0.7143
***callbacks***
saving losses to model_2/losses.log

Epoch 4: val_loss improved from 0.94499 to 0.90047, saving model to model_2/KERAS_check_best_model.h5

Epoch 4: val_loss improved from 0.94499 to 0.90047, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 4: saving model to model_2/KERAS_check_model_last.h5

Epoch 4: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.9183 - accuracy: 0.7143 - val_loss: 0.9005 - val_accuracy: 0.7187 - lr: 1.0000e-04
Epoch 5/30
479/487 [============================&gt;.] - ETA: 0s - loss: 1.0761 - accuracy: 0.6410
***callbacks***
saving losses to model_2/losses.log

Epoch 5: val_loss did not improve from 0.90047

Epoch 5: val_loss did not improve from 0.90047

Epoch 5: saving model to model_2/KERAS_check_model_last.h5

Epoch 5: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 1.0749 - accuracy: 0.6415 - val_loss: 0.9990 - val_accuracy: 0.6722 - lr: 1.0000e-04
Epoch 6/30
467/487 [===========================&gt;..] - ETA: 0s - loss: 0.9622 - accuracy: 0.6860
***callbacks***
saving losses to model_2/losses.log

Epoch 6: val_loss did not improve from 0.90047

Epoch 6: val_loss did not improve from 0.90047

Epoch 6: saving model to model_2/KERAS_check_model_last.h5

Epoch 6: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.9614 - accuracy: 0.6864 - val_loss: 0.9364 - val_accuracy: 0.6952 - lr: 1.0000e-04
Epoch 7/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.9161 - accuracy: 0.7013
***callbacks***
saving losses to model_2/losses.log

Epoch 7: val_loss did not improve from 0.90047

Epoch 7: val_loss did not improve from 0.90047

Epoch 7: saving model to model_2/KERAS_check_model_last.h5

Epoch 7: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.9157 - accuracy: 0.7015 - val_loss: 0.9021 - val_accuracy: 0.7052 - lr: 1.0000e-04
Epoch 8/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.8868 - accuracy: 0.7100
***callbacks***
saving losses to model_2/losses.log

Epoch 8: val_loss improved from 0.90047 to 0.87748, saving model to model_2/KERAS_check_best_model.h5

Epoch 8: val_loss improved from 0.90047 to 0.87748, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 8: saving model to model_2/KERAS_check_model_last.h5

Epoch 8: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.8865 - accuracy: 0.7101 - val_loss: 0.8775 - val_accuracy: 0.7122 - lr: 1.0000e-04
Epoch 9/30
472/487 [============================&gt;.] - ETA: 0s - loss: 0.8659 - accuracy: 0.7155
***callbacks***
saving losses to model_2/losses.log

Epoch 9: val_loss improved from 0.87748 to 0.86037, saving model to model_2/KERAS_check_best_model.h5

Epoch 9: val_loss improved from 0.87748 to 0.86037, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 9: saving model to model_2/KERAS_check_model_last.h5

Epoch 9: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.8655 - accuracy: 0.7156 - val_loss: 0.8604 - val_accuracy: 0.7173 - lr: 1.0000e-04
Epoch 10/30
473/487 [============================&gt;.] - ETA: 0s - loss: 0.8507 - accuracy: 0.7193
***callbacks***
saving losses to model_2/losses.log

Epoch 10: val_loss improved from 0.86037 to 0.84772, saving model to model_2/KERAS_check_best_model.h5

Epoch 10: val_loss improved from 0.86037 to 0.84772, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 10: saving model to model_2/KERAS_check_model_last.h5

Epoch 10: saving model to model_2/KERAS_check_model_last_weights.h5

Epoch 10: saving model to model_2/KERAS_check_model_epoch10.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.8506 - accuracy: 0.7192 - val_loss: 0.8477 - val_accuracy: 0.7208 - lr: 1.0000e-04
Epoch 11/30
485/487 [============================&gt;.] - ETA: 0s - loss: 0.8392 - accuracy: 0.7219
***callbacks***
saving losses to model_2/losses.log

Epoch 11: val_loss improved from 0.84772 to 0.83780, saving model to model_2/KERAS_check_best_model.h5

Epoch 11: val_loss improved from 0.84772 to 0.83780, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 11: saving model to model_2/KERAS_check_model_last.h5

Epoch 11: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.8392 - accuracy: 0.7219 - val_loss: 0.8378 - val_accuracy: 0.7228 - lr: 1.0000e-04
Epoch 12/30
477/487 [============================&gt;.] - ETA: 0s - loss: 0.8296 - accuracy: 0.7244
***callbacks***
saving losses to model_2/losses.log

Epoch 12: val_loss improved from 0.83780 to 0.82951, saving model to model_2/KERAS_check_best_model.h5

Epoch 12: val_loss improved from 0.83780 to 0.82951, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 12: saving model to model_2/KERAS_check_model_last.h5

Epoch 12: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.8299 - accuracy: 0.7243 - val_loss: 0.8295 - val_accuracy: 0.7256 - lr: 1.0000e-04
Epoch 13/30
474/487 [============================&gt;.] - ETA: 0s - loss: 0.8224 - accuracy: 0.7262
***callbacks***
saving losses to model_2/losses.log

Epoch 13: val_loss improved from 0.82951 to 0.82223, saving model to model_2/KERAS_check_best_model.h5

Epoch 13: val_loss improved from 0.82951 to 0.82223, saving model to model_2/KERAS_check_best_model_weights.h5
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 13: saving model to model_2/KERAS_check_model_last.h5

Epoch 13: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.8220 - accuracy: 0.7263 - val_loss: 0.8222 - val_accuracy: 0.7271 - lr: 1.0000e-04
Epoch 14/30
481/487 [============================&gt;.] - ETA: 0s - loss: 0.8151 - accuracy: 0.7282
***callbacks***
saving losses to model_2/losses.log

Epoch 14: val_loss improved from 0.82223 to 0.81565, saving model to model_2/KERAS_check_best_model.h5

Epoch 14: val_loss improved from 0.82223 to 0.81565, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 14: saving model to model_2/KERAS_check_model_last.h5

Epoch 14: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.8150 - accuracy: 0.7282 - val_loss: 0.8157 - val_accuracy: 0.7286 - lr: 1.0000e-04
Epoch 15/30
467/487 [===========================&gt;..] - ETA: 0s - loss: 0.8090 - accuracy: 0.7298
***callbacks***
saving losses to model_2/losses.log

Epoch 15: val_loss improved from 0.81565 to 0.80983, saving model to model_2/KERAS_check_best_model.h5

Epoch 15: val_loss improved from 0.81565 to 0.80983, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 15: saving model to model_2/KERAS_check_model_last.h5

Epoch 15: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.8087 - accuracy: 0.7298 - val_loss: 0.8098 - val_accuracy: 0.7301 - lr: 1.0000e-04
Epoch 16/30
464/487 [===========================&gt;..] - ETA: 0s - loss: 0.8029 - accuracy: 0.7315
***callbacks***
saving losses to model_2/losses.log

Epoch 16: val_loss improved from 0.80983 to 0.80443, saving model to model_2/KERAS_check_best_model.h5

Epoch 16: val_loss improved from 0.80983 to 0.80443, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 16: saving model to model_2/KERAS_check_model_last.h5

Epoch 16: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.8030 - accuracy: 0.7314 - val_loss: 0.8044 - val_accuracy: 0.7314 - lr: 1.0000e-04
Epoch 17/30
461/487 [===========================&gt;..] - ETA: 0s - loss: 0.7980 - accuracy: 0.7325
***callbacks***
saving losses to model_2/losses.log

Epoch 17: val_loss improved from 0.80443 to 0.79963, saving model to model_2/KERAS_check_best_model.h5

Epoch 17: val_loss improved from 0.80443 to 0.79963, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 17: saving model to model_2/KERAS_check_model_last.h5

Epoch 17: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.7977 - accuracy: 0.7325 - val_loss: 0.7996 - val_accuracy: 0.7324 - lr: 1.0000e-04
Epoch 18/30
471/487 [============================&gt;.] - ETA: 0s - loss: 0.7925 - accuracy: 0.7340
***callbacks***
saving losses to model_2/losses.log

Epoch 18: val_loss improved from 0.79963 to 0.79510, saving model to model_2/KERAS_check_best_model.h5

Epoch 18: val_loss improved from 0.79963 to 0.79510, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 18: saving model to model_2/KERAS_check_model_last.h5

Epoch 18: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.7930 - accuracy: 0.7338 - val_loss: 0.7951 - val_accuracy: 0.7337 - lr: 1.0000e-04
Epoch 19/30
471/487 [============================&gt;.] - ETA: 0s - loss: 0.7889 - accuracy: 0.7348
***callbacks***
saving losses to model_2/losses.log

Epoch 19: val_loss improved from 0.79510 to 0.79102, saving model to model_2/KERAS_check_best_model.h5

Epoch 19: val_loss improved from 0.79510 to 0.79102, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 19: saving model to model_2/KERAS_check_model_last.h5

Epoch 19: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.7886 - accuracy: 0.7348 - val_loss: 0.7910 - val_accuracy: 0.7345 - lr: 1.0000e-04
Epoch 20/30
472/487 [============================&gt;.] - ETA: 0s - loss: 0.7845 - accuracy: 0.7358
***callbacks***
saving losses to model_2/losses.log

Epoch 20: val_loss improved from 0.79102 to 0.78685, saving model to model_2/KERAS_check_best_model.h5

Epoch 20: val_loss improved from 0.79102 to 0.78685, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 20: saving model to model_2/KERAS_check_model_last.h5

Epoch 20: saving model to model_2/KERAS_check_model_last_weights.h5

Epoch 20: saving model to model_2/KERAS_check_model_epoch20.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7844 - accuracy: 0.7358 - val_loss: 0.7869 - val_accuracy: 0.7357 - lr: 1.0000e-04
Epoch 21/30
466/487 [===========================&gt;..] - ETA: 0s - loss: 0.7807 - accuracy: 0.7370
***callbacks***
saving losses to model_2/losses.log

Epoch 21: val_loss improved from 0.78685 to 0.78309, saving model to model_2/KERAS_check_best_model.h5

Epoch 21: val_loss improved from 0.78685 to 0.78309, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 21: saving model to model_2/KERAS_check_model_last.h5

Epoch 21: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7805 - accuracy: 0.7371 - val_loss: 0.7831 - val_accuracy: 0.7368 - lr: 1.0000e-04
Epoch 22/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.7769 - accuracy: 0.7382
***callbacks***
saving losses to model_2/losses.log

Epoch 22: val_loss improved from 0.78309 to 0.77970, saving model to model_2/KERAS_check_best_model.h5

Epoch 22: val_loss improved from 0.78309 to 0.77970, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 22: saving model to model_2/KERAS_check_model_last.h5

Epoch 22: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7768 - accuracy: 0.7382 - val_loss: 0.7797 - val_accuracy: 0.7377 - lr: 1.0000e-04
Epoch 23/30
470/487 [===========================&gt;..] - ETA: 0s - loss: 0.7740 - accuracy: 0.7392
***callbacks***
saving losses to model_2/losses.log

Epoch 23: val_loss improved from 0.77970 to 0.77655, saving model to model_2/KERAS_check_best_model.h5

Epoch 23: val_loss improved from 0.77970 to 0.77655, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 23: saving model to model_2/KERAS_check_model_last.h5

Epoch 23: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7736 - accuracy: 0.7393 - val_loss: 0.7766 - val_accuracy: 0.7386 - lr: 1.0000e-04
Epoch 24/30
487/487 [==============================] - ETA: 0s - loss: 0.7706 - accuracy: 0.7403
***callbacks***
saving losses to model_2/losses.log

Epoch 24: val_loss improved from 0.77655 to 0.77368, saving model to model_2/KERAS_check_best_model.h5

Epoch 24: val_loss improved from 0.77655 to 0.77368, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 24: saving model to model_2/KERAS_check_model_last.h5

Epoch 24: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7706 - accuracy: 0.7403 - val_loss: 0.7737 - val_accuracy: 0.7399 - lr: 1.0000e-04
Epoch 25/30
472/487 [============================&gt;.] - ETA: 0s - loss: 0.7679 - accuracy: 0.7411
***callbacks***
saving losses to model_2/losses.log

Epoch 25: val_loss improved from 0.77368 to 0.77119, saving model to model_2/KERAS_check_best_model.h5

Epoch 25: val_loss improved from 0.77368 to 0.77119, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 25: saving model to model_2/KERAS_check_model_last.h5

Epoch 25: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7678 - accuracy: 0.7412 - val_loss: 0.7712 - val_accuracy: 0.7405 - lr: 1.0000e-04
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 26/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.7651 - accuracy: 0.7421
***callbacks***
saving losses to model_2/losses.log

Epoch 26: val_loss improved from 0.77119 to 0.76868, saving model to model_2/KERAS_check_best_model.h5

Epoch 26: val_loss improved from 0.77119 to 0.76868, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 26: saving model to model_2/KERAS_check_model_last.h5

Epoch 26: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7653 - accuracy: 0.7420 - val_loss: 0.7687 - val_accuracy: 0.7416 - lr: 1.0000e-04
Epoch 27/30
470/487 [===========================&gt;..] - ETA: 0s - loss: 0.7633 - accuracy: 0.7428
***callbacks***
saving losses to model_2/losses.log

Epoch 27: val_loss improved from 0.76868 to 0.76654, saving model to model_2/KERAS_check_best_model.h5

Epoch 27: val_loss improved from 0.76868 to 0.76654, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 27: saving model to model_2/KERAS_check_model_last.h5

Epoch 27: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7629 - accuracy: 0.7429 - val_loss: 0.7665 - val_accuracy: 0.7427 - lr: 1.0000e-04
Epoch 28/30
472/487 [============================&gt;.] - ETA: 0s - loss: 0.7608 - accuracy: 0.7435
***callbacks***
saving losses to model_2/losses.log

Epoch 28: val_loss improved from 0.76654 to 0.76441, saving model to model_2/KERAS_check_best_model.h5

Epoch 28: val_loss improved from 0.76654 to 0.76441, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 28: saving model to model_2/KERAS_check_model_last.h5

Epoch 28: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.7607 - accuracy: 0.7435 - val_loss: 0.7644 - val_accuracy: 0.7427 - lr: 1.0000e-04
Epoch 29/30
461/487 [===========================&gt;..] - ETA: 0s - loss: 0.7586 - accuracy: 0.7444
***callbacks***
saving losses to model_2/losses.log

Epoch 29: val_loss improved from 0.76441 to 0.76222, saving model to model_2/KERAS_check_best_model.h5

Epoch 29: val_loss improved from 0.76441 to 0.76222, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 29: saving model to model_2/KERAS_check_model_last.h5

Epoch 29: saving model to model_2/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7586 - accuracy: 0.7443 - val_loss: 0.7622 - val_accuracy: 0.7439 - lr: 1.0000e-04
Epoch 30/30
475/487 [============================&gt;.] - ETA: 0s - loss: 0.7568 - accuracy: 0.7448
***callbacks***
saving losses to model_2/losses.log

Epoch 30: val_loss improved from 0.76222 to 0.76032, saving model to model_2/KERAS_check_best_model.h5

Epoch 30: val_loss improved from 0.76222 to 0.76032, saving model to model_2/KERAS_check_best_model_weights.h5

Epoch 30: saving model to model_2/KERAS_check_model_last.h5

Epoch 30: saving model to model_2/KERAS_check_model_last_weights.h5

Epoch 30: saving model to model_2/KERAS_check_model_epoch30.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.7566 - accuracy: 0.7449 - val_loss: 0.7603 - val_accuracy: 0.7443 - lr: 1.0000e-04
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
</pre></div>
</div>
</div>
</div>
</section>
<section id="check-sparsity">
<h2>Check sparsity<a class="headerlink" href="#check-sparsity" title="Permalink to this headline">#</a></h2>
<p>Make a quick check that the model was indeed trained sparse. We’ll just make a histogram of the weights of the 1st layer, and hopefully observe a large peak in the bin containing ‘0’. Note logarithmic y axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">h</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">b</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">h</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">% o</span><span class="s1">f zeros = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>% of zeros = 0.75
</pre></div>
</div>
<img alt="_images/2.3_compression_12_1.png" src="_images/2.3_compression_12_1.png" />
</div>
</div>
</section>
<section id="check-performance">
<h2>Check performance<a class="headerlink" href="#check-performance" title="Permalink to this headline">#</a></h2>
<p>How does this 75% sparse model compare against the unpruned model? Let’s report the accuracy and make a ROC curve. The pruned model is shown with solid lines, the unpruned model from part 1 is shown with dashed lines.
<strong>Make sure you’ve trained the model from part 1</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="n">model_ref</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;model_1/KERAS_check_best_model.h5&#39;</span><span class="p">)</span>

<span class="n">y_ref</span> <span class="o">=</span> <span class="n">model_ref</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prune</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy unpruned: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_ref</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy pruned:   </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_prune</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_ref</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prune</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>
<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">),</span>
         <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)]</span>
<span class="kn">from</span> <span class="nn">matplotlib.legend</span> <span class="kn">import</span> <span class="n">Legend</span>
<span class="n">leg</span> <span class="o">=</span> <span class="n">Legend</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">lines</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unpruned&#39;</span><span class="p">,</span> <span class="s1">&#39;pruned&#39;</span><span class="p">],</span>
            <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">leg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5188/5188 [==============================] - 3s 571us/step
5188/5188 [==============================] - 3s 541us/step
Accuracy unpruned: 0.7502650602409638
Accuracy pruned:   0.7430722891566265
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x161184a60&gt;
</pre></div>
</div>
<img alt="_images/2.3_compression_14_2.png" src="_images/2.3_compression_14_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow_model_optimization.python.core.sparsity.keras</span> <span class="kn">import</span> <span class="n">prune</span><span class="p">,</span> <span class="n">pruning_callbacks</span><span class="p">,</span> <span class="n">pruning_schedule</span>
<span class="kn">from</span> <span class="nn">tensorflow_model_optimization.sparsity.keras</span> <span class="kn">import</span> <span class="n">strip_pruning</span>
<span class="n">pruning_params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;pruning_schedule&quot;</span> <span class="p">:</span> <span class="n">pruning_schedule</span><span class="o">.</span><span class="n">ConstantSparsity</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">begin_step</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="mi">100</span><span class="p">)}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prune</span><span class="o">.</span><span class="n">prune_low_magnitude</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">pruning_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:212: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  mask = self.add_variable(
/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:219: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  threshold = self.add_variable(
/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:233: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.
  self.pruning_step = self.add_variable(
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h2>Train the model<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<p>We’ll use the same settings as the model for part 1: Adam optimizer with categorical crossentropy loss.
The callbacks will decay the learning rate and save the model into a directory ‘model_2’
The model isn’t very complex, so this should just take a few minutes even on the CPU.
If you’ve restarted the notebook kernel after training once, set <code class="docutils literal notranslate"><span class="pre">train</span> <span class="pre">=</span> <span class="pre">False</span></code> to load the trained model rather than training again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">train</span><span class="p">:</span>
    <span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">],</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">all_callbacks</span><span class="p">(</span><span class="n">stop_patience</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                              <span class="n">lr_factor</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                              <span class="n">lr_patience</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                              <span class="n">lr_epsilon</span> <span class="o">=</span> <span class="mf">0.000001</span><span class="p">,</span>
                              <span class="n">lr_cooldown</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                              <span class="n">lr_minimum</span> <span class="o">=</span> <span class="mf">0.0000001</span><span class="p">,</span>
                              <span class="n">outputDir</span> <span class="o">=</span> <span class="s1">&#39;model_3&#39;</span><span class="p">)</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pruning_callbacks</span><span class="o">.</span><span class="n">UpdatePruningStep</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_val</span><span class="p">,</span> <span class="n">y_train_val</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
              <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">callbacks</span> <span class="o">=</span> <span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="p">)</span>
    <span class="c1"># Save the model again but with the pruning &#39;stripped&#39; to use the regular layer types</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">strip_pruning</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;model_3/KERAS_check_best_model.h5&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;model_3/KERAS_check_best_model.h5&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
Epoch 1/30
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/drankin/miniconda3/envs/ml-latest/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  1/487 [..............................] - ETA: 11:45 - loss: 0.7496 - accuracy: 0.7529WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0049s). Check your callbacks.
484/487 [============================&gt;.] - ETA: 0s - loss: 0.7539 - accuracy: 0.7458
***callbacks***
saving losses to model_3/losses.log

Epoch 1: val_loss improved from inf to 0.75636, saving model to model_3/KERAS_check_best_model.h5

Epoch 1: val_loss improved from inf to 0.75636, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 1: saving model to model_3/KERAS_check_model_last.h5

Epoch 1: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 3ms/step - loss: 0.7538 - accuracy: 0.7459 - val_loss: 0.7564 - val_accuracy: 0.7457 - lr: 1.0000e-04
Epoch 2/30
481/487 [============================&gt;.] - ETA: 0s - loss: 0.7502 - accuracy: 0.7470
***callbacks***
saving losses to model_3/losses.log

Epoch 2: val_loss improved from 0.75636 to 0.75299, saving model to model_3/KERAS_check_best_model.h5

Epoch 2: val_loss improved from 0.75636 to 0.75299, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 2: saving model to model_3/KERAS_check_model_last.h5

Epoch 2: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.7502 - accuracy: 0.7470 - val_loss: 0.7530 - val_accuracy: 0.7469 - lr: 1.0000e-04
Epoch 3/30
484/487 [============================&gt;.] - ETA: 0s - loss: 0.7471 - accuracy: 0.7478
***callbacks***
saving losses to model_3/losses.log

Epoch 3: val_loss improved from 0.75299 to 0.75039, saving model to model_3/KERAS_check_best_model.h5

Epoch 3: val_loss improved from 0.75299 to 0.75039, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 3: saving model to model_3/KERAS_check_model_last.h5

Epoch 3: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.7471 - accuracy: 0.7478 - val_loss: 0.7504 - val_accuracy: 0.7479 - lr: 1.0000e-04
Epoch 4/30
468/487 [===========================&gt;..] - ETA: 0s - loss: 0.7439 - accuracy: 0.7489
***callbacks***
saving losses to model_3/losses.log

Epoch 4: val_loss improved from 0.75039 to 0.74776, saving model to model_3/KERAS_check_best_model.h5

Epoch 4: val_loss improved from 0.75039 to 0.74776, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 4: saving model to model_3/KERAS_check_model_last.h5

Epoch 4: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.7444 - accuracy: 0.7487 - val_loss: 0.7478 - val_accuracy: 0.7489 - lr: 1.0000e-04
Epoch 5/30
474/487 [============================&gt;.] - ETA: 0s - loss: 1.2968 - accuracy: 0.5169
***callbacks***
saving losses to model_3/losses.log

Epoch 5: val_loss did not improve from 0.74776

Epoch 5: val_loss did not improve from 0.74776

Epoch 5: saving model to model_3/KERAS_check_model_last.h5

Epoch 5: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 1.2964 - accuracy: 0.5172 - val_loss: 1.2874 - val_accuracy: 0.5295 - lr: 1.0000e-04
Epoch 6/30
466/487 [===========================&gt;..] - ETA: 0s - loss: 1.2403 - accuracy: 0.5583
***callbacks***
saving losses to model_3/losses.log

Epoch 6: val_loss did not improve from 0.74776

Epoch 6: val_loss did not improve from 0.74776

Epoch 6: saving model to model_3/KERAS_check_model_last.h5

Epoch 6: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 1.2387 - accuracy: 0.5585 - val_loss: 1.1998 - val_accuracy: 0.5639 - lr: 1.0000e-04
Epoch 7/30
464/487 [===========================&gt;..] - ETA: 0s - loss: 1.1686 - accuracy: 0.5711
***callbacks***
saving losses to model_3/losses.log

Epoch 7: val_loss did not improve from 0.74776

Epoch 7: val_loss did not improve from 0.74776

Epoch 7: saving model to model_3/KERAS_check_model_last.h5

Epoch 7: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 1.1676 - accuracy: 0.5712 - val_loss: 1.1442 - val_accuracy: 0.5717 - lr: 1.0000e-04
Epoch 8/30
476/487 [============================&gt;.] - ETA: 0s - loss: 1.1258 - accuracy: 0.5774
***callbacks***
saving losses to model_3/losses.log

Epoch 8: val_loss did not improve from 0.74776

Epoch 8: val_loss did not improve from 0.74776

Epoch 8: saving model to model_3/KERAS_check_model_last.h5

Epoch 8: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 1.1250 - accuracy: 0.5775 - val_loss: 1.1119 - val_accuracy: 0.5768 - lr: 1.0000e-04
Epoch 9/30
479/487 [============================&gt;.] - ETA: 0s - loss: 1.0977 - accuracy: 0.5821
***callbacks***
saving losses to model_3/losses.log

Epoch 9: val_loss did not improve from 0.74776

Epoch 9: val_loss did not improve from 0.74776

Epoch 9: saving model to model_3/KERAS_check_model_last.h5

Epoch 9: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 1.0974 - accuracy: 0.5823 - val_loss: 1.0883 - val_accuracy: 0.5809 - lr: 1.0000e-04
Epoch 10/30
475/487 [============================&gt;.] - ETA: 0s - loss: 1.0767 - accuracy: 0.5858
***callbacks***
saving losses to model_3/losses.log

Epoch 10: val_loss did not improve from 0.74776

Epoch 10: val_loss did not improve from 0.74776

Epoch 10: saving model to model_3/KERAS_check_model_last.h5

Epoch 10: saving model to model_3/KERAS_check_model_last_weights.h5

Epoch 10: saving model to model_3/KERAS_check_model_epoch10.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 1.0769 - accuracy: 0.5857 - val_loss: 1.0706 - val_accuracy: 0.5837 - lr: 1.0000e-04
Epoch 11/30
475/487 [============================&gt;.] - ETA: 0s - loss: 1.0611 - accuracy: 0.5880
***callbacks***
saving losses to model_3/losses.log

Epoch 11: val_loss did not improve from 0.74776

Epoch 11: val_loss did not improve from 0.74776

Epoch 11: saving model to model_3/KERAS_check_model_last.h5

Epoch 11: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 1.0609 - accuracy: 0.5879 - val_loss: 1.0562 - val_accuracy: 0.5860 - lr: 1.0000e-04
Epoch 12/30
476/487 [============================&gt;.] - ETA: 0s - loss: 1.0474 - accuracy: 0.5895
***callbacks***
saving losses to model_3/losses.log

Epoch 12: val_loss did not improve from 0.74776

Epoch 12: val_loss did not improve from 0.74776

Epoch 12: saving model to model_3/KERAS_check_model_last.h5

Epoch 12: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 1.0476 - accuracy: 0.5896 - val_loss: 1.0439 - val_accuracy: 0.5875 - lr: 1.0000e-04
Epoch 13/30
487/487 [==============================] - ETA: 0s - loss: 1.0360 - accuracy: 0.5907
***callbacks***
saving losses to model_3/losses.log

Epoch 13: val_loss did not improve from 0.74776

Epoch 13: val_loss did not improve from 0.74776

Epoch 13: saving model to model_3/KERAS_check_model_last.h5

Epoch 13: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 1.0360 - accuracy: 0.5907 - val_loss: 1.0329 - val_accuracy: 0.5885 - lr: 1.0000e-04
Epoch 14/30
480/487 [============================&gt;.] - ETA: 0s - loss: 1.0249 - accuracy: 0.5915
***callbacks***
saving losses to model_3/losses.log

Epoch 14: val_loss did not improve from 0.74776

Epoch 14: val_loss did not improve from 0.74776

Epoch 14: saving model to model_3/KERAS_check_model_last.h5

Epoch 14: saving model to model_3/KERAS_check_model_last_weights.h5
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 1.0248 - accuracy: 0.5914 - val_loss: 1.0214 - val_accuracy: 0.5893 - lr: 1.0000e-04
Epoch 15/30
482/487 [============================&gt;.] - ETA: 0s - loss: 1.0160 - accuracy: 0.5920
***callbacks***
saving losses to model_3/losses.log

Epoch 15: val_loss did not improve from 0.74776

Epoch 15: val_loss did not improve from 0.74776

Epoch 15: saving model to model_3/KERAS_check_model_last.h5

Epoch 15: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 1.0161 - accuracy: 0.5919 - val_loss: 1.0156 - val_accuracy: 0.5896 - lr: 5.0000e-05
Epoch 16/30
487/487 [==============================] - ETA: 0s - loss: 1.0104 - accuracy: 0.5923
***callbacks***
saving losses to model_3/losses.log

Epoch 16: val_loss did not improve from 0.74776

Epoch 16: val_loss did not improve from 0.74776

Epoch 16: saving model to model_3/KERAS_check_model_last.h5

Epoch 16: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 1.0104 - accuracy: 0.5923 - val_loss: 1.0101 - val_accuracy: 0.5903 - lr: 5.0000e-05
Epoch 17/30
467/487 [===========================&gt;..] - ETA: 0s - loss: 1.0050 - accuracy: 0.5926
***callbacks***
saving losses to model_3/losses.log

Epoch 17: val_loss did not improve from 0.74776

Epoch 17: val_loss did not improve from 0.74776

Epoch 17: saving model to model_3/KERAS_check_model_last.h5

Epoch 17: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 1.0050 - accuracy: 0.5927 - val_loss: 1.0048 - val_accuracy: 0.5907 - lr: 5.0000e-05
Epoch 18/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.9997 - accuracy: 0.5932
***callbacks***
saving losses to model_3/losses.log

Epoch 18: val_loss did not improve from 0.74776

Epoch 18: val_loss did not improve from 0.74776

Epoch 18: saving model to model_3/KERAS_check_model_last.h5

Epoch 18: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9999 - accuracy: 0.5931 - val_loss: 0.9998 - val_accuracy: 0.5912 - lr: 5.0000e-05
Epoch 19/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.9950 - accuracy: 0.5935
***callbacks***
saving losses to model_3/losses.log

Epoch 19: val_loss did not improve from 0.74776

Epoch 19: val_loss did not improve from 0.74776

Epoch 19: saving model to model_3/KERAS_check_model_last.h5

Epoch 19: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9950 - accuracy: 0.5935 - val_loss: 0.9950 - val_accuracy: 0.5915 - lr: 5.0000e-05
Epoch 20/30
487/487 [==============================] - ETA: 0s - loss: 0.9903 - accuracy: 0.5939
***callbacks***
saving losses to model_3/losses.log

Epoch 20: val_loss did not improve from 0.74776

Epoch 20: val_loss did not improve from 0.74776

Epoch 20: saving model to model_3/KERAS_check_model_last.h5

Epoch 20: saving model to model_3/KERAS_check_model_last_weights.h5

Epoch 20: saving model to model_3/KERAS_check_model_epoch20.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9903 - accuracy: 0.5939 - val_loss: 0.9903 - val_accuracy: 0.5918 - lr: 5.0000e-05
Epoch 21/30
472/487 [============================&gt;.] - ETA: 0s - loss: 0.9852 - accuracy: 0.5945
***callbacks***
saving losses to model_3/losses.log

Epoch 21: val_loss did not improve from 0.74776

Epoch 21: val_loss did not improve from 0.74776

Epoch 21: saving model to model_3/KERAS_check_model_last.h5

Epoch 21: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9853 - accuracy: 0.5945 - val_loss: 0.9851 - val_accuracy: 0.5927 - lr: 5.0000e-05
Epoch 22/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.9802 - accuracy: 0.5953
***callbacks***
saving losses to model_3/losses.log

Epoch 22: val_loss did not improve from 0.74776

Epoch 22: val_loss did not improve from 0.74776

Epoch 22: saving model to model_3/KERAS_check_model_last.h5

Epoch 22: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9802 - accuracy: 0.5953 - val_loss: 0.9800 - val_accuracy: 0.5935 - lr: 5.0000e-05
Epoch 23/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.9752 - accuracy: 0.5960
***callbacks***
saving losses to model_3/losses.log

Epoch 23: val_loss did not improve from 0.74776

Epoch 23: val_loss did not improve from 0.74776

Epoch 23: saving model to model_3/KERAS_check_model_last.h5

Epoch 23: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9751 - accuracy: 0.5961 - val_loss: 0.9751 - val_accuracy: 0.5943 - lr: 5.0000e-05
Epoch 24/30
487/487 [==============================] - ETA: 0s - loss: 0.9702 - accuracy: 0.5967
***callbacks***
saving losses to model_3/losses.log

Epoch 24: val_loss did not improve from 0.74776

Epoch 24: val_loss did not improve from 0.74776

Epoch 24: saving model to model_3/KERAS_check_model_last.h5

Epoch 24: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9702 - accuracy: 0.5967 - val_loss: 0.9703 - val_accuracy: 0.5949 - lr: 5.0000e-05
Epoch 25/30
485/487 [============================&gt;.] - ETA: 0s - loss: 0.9656 - accuracy: 0.5976
***callbacks***
saving losses to model_3/losses.log

Epoch 25: val_loss did not improve from 0.74776

Epoch 25: val_loss did not improve from 0.74776

Epoch 25: saving model to model_3/KERAS_check_model_last.h5

Epoch 25: saving model to model_3/KERAS_check_model_last_weights.h5

Epoch 25: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9654 - accuracy: 0.5977 - val_loss: 0.9655 - val_accuracy: 0.5963 - lr: 5.0000e-05
Epoch 26/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.9618 - accuracy: 0.5990
***callbacks***
saving losses to model_3/losses.log

Epoch 26: val_loss did not improve from 0.74776

Epoch 26: val_loss did not improve from 0.74776

Epoch 26: saving model to model_3/KERAS_check_model_last.h5

Epoch 26: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9618 - accuracy: 0.5990 - val_loss: 0.9631 - val_accuracy: 0.5975 - lr: 2.5000e-05
Epoch 27/30
461/487 [===========================&gt;..] - ETA: 0s - loss: 0.9595 - accuracy: 0.6000
***callbacks***
saving losses to model_3/losses.log

Epoch 27: val_loss did not improve from 0.74776

Epoch 27: val_loss did not improve from 0.74776

Epoch 27: saving model to model_3/KERAS_check_model_last.h5

Epoch 27: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9595 - accuracy: 0.6002 - val_loss: 0.9608 - val_accuracy: 0.5989 - lr: 2.5000e-05
Epoch 28/30
466/487 [===========================&gt;..] - ETA: 0s - loss: 0.9573 - accuracy: 0.6018
***callbacks***
saving losses to model_3/losses.log

Epoch 28: val_loss did not improve from 0.74776

Epoch 28: val_loss did not improve from 0.74776

Epoch 28: saving model to model_3/KERAS_check_model_last.h5

Epoch 28: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 2ms/step - loss: 0.9572 - accuracy: 0.6016 - val_loss: 0.9585 - val_accuracy: 0.6003 - lr: 2.5000e-05
Epoch 29/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.9549 - accuracy: 0.6031
***callbacks***
saving losses to model_3/losses.log

Epoch 29: val_loss did not improve from 0.74776
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 29: val_loss did not improve from 0.74776

Epoch 29: saving model to model_3/KERAS_check_model_last.h5

Epoch 29: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.9549 - accuracy: 0.6031 - val_loss: 0.9563 - val_accuracy: 0.6018 - lr: 2.5000e-05
Epoch 30/30
469/487 [===========================&gt;..] - ETA: 0s - loss: 0.9527 - accuracy: 0.6047
***callbacks***
saving losses to model_3/losses.log

Epoch 30: val_loss did not improve from 0.74776

Epoch 30: val_loss did not improve from 0.74776

Epoch 30: saving model to model_3/KERAS_check_model_last.h5

Epoch 30: saving model to model_3/KERAS_check_model_last_weights.h5

Epoch 30: saving model to model_3/KERAS_check_model_epoch30.h5

***callbacks end***

487/487 [==============================] - 1s 3ms/step - loss: 0.9527 - accuracy: 0.6047 - val_loss: 0.9541 - val_accuracy: 0.6034 - lr: 2.5000e-05
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h2>Check sparsity<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p>Make a quick check that the model was indeed trained sparse. We’ll just make a histogram of the weights of the 1st layer, and hopefully observe a large peak in the bin containing ‘0’. Note logarithmic y axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">h</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">b</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">h</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">% o</span><span class="s1">f zeros = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>% of zeros = 0.9501953125
</pre></div>
</div>
<img alt="_images/2.3_compression_19_1.png" src="_images/2.3_compression_19_1.png" />
</div>
</div>
</section>
<section id="id3">
<h2>Check performance<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h2>
<p>How does this 75% sparse model compare against the unpruned model? Let’s report the accuracy and make a ROC curve. The pruned model is shown with solid lines, the unpruned model from part 1 is shown with dashed lines.
<strong>Make sure you’ve trained the model from part 1</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="n">model_ref</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;model_1/KERAS_check_best_model.h5&#39;</span><span class="p">)</span>
<span class="n">model_prune75</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;model_2/KERAS_check_best_model.h5&#39;</span><span class="p">)</span>

<span class="n">y_ref</span> <span class="o">=</span> <span class="n">model_ref</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prune75</span> <span class="o">=</span> <span class="n">model_prune75</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prune95</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy unpruned:     </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_ref</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy pruned (75%): </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_prune75</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy pruned (95%): </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_prune95</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_ref</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prune75</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prune95</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>
<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">),</span>
         <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">),</span>
         <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">)]</span>
<span class="kn">from</span> <span class="nn">matplotlib.legend</span> <span class="kn">import</span> <span class="n">Legend</span>
<span class="n">leg</span> <span class="o">=</span> <span class="n">Legend</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">lines</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unpruned&#39;</span><span class="p">,</span> <span class="s1">&#39;pruned (75%)&#39;</span><span class="p">,</span> <span class="s1">&#39;pruned (95%)&#39;</span><span class="p">],</span>
            <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">leg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
5188/5188 [==============================] - 3s 661us/step
5188/5188 [==============================] - 3s 571us/step
5188/5188 [==============================] - 3s 585us/step
Accuracy unpruned:     0.7502650602409638
Accuracy pruned (75%): 0.7430722891566265
Accuracy pruned (95%): 0.6046626506024096
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x1635c0d30&gt;
</pre></div>
</div>
<img alt="_images/2.3_compression_21_2.png" src="_images/2.3_compression_21_2.png" />
</div>
</div>
</section>
<section id="convert-the-model-to-fpga-firmware-with-hls4ml">
<h2>Convert the model to FPGA firmware with hls4ml<a class="headerlink" href="#convert-the-model-to-fpga-firmware-with-hls4ml" title="Permalink to this headline">#</a></h2>
<p>Let’s use the default configuration: <code class="docutils literal notranslate"><span class="pre">ap_fixed&lt;16,6&gt;</span></code> precision everywhere and <code class="docutils literal notranslate"><span class="pre">ReuseFactor=1</span></code>, so we can compare with the part 1 model. We need to use <code class="docutils literal notranslate"><span class="pre">strip_pruning</span></code> to change the layer types back to their originals.</p>
<p><strong>The synthesis will take a while</strong></p>
<p>While the C-Synthesis is running, we can monitor the progress looking at the log file by opening a terminal from the notebook home, and executing:</p>
<p><code class="docutils literal notranslate"><span class="pre">tail</span> <span class="pre">-f</span> <span class="pre">model_2/hls4ml_prj/vivado_hls.log</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">hls4ml</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">hls4ml</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">config_from_keras_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">granularity</span><span class="o">=</span><span class="s1">&#39;model&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">hls_model</span> <span class="o">=</span> <span class="n">hls4ml</span><span class="o">.</span><span class="n">converters</span><span class="o">.</span><span class="n">convert_from_keras_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                                       <span class="n">hls_config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
                                                       <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;model_2/hls4ml_prj&#39;</span><span class="p">,</span>
                                                       <span class="n">part</span><span class="o">=</span><span class="s1">&#39;xcu250-figd2104-2L-e&#39;</span><span class="p">)</span>
<span class="n">hls_model</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="n">hls_model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">csim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="check-the-reports">
<h2>Check the reports<a class="headerlink" href="#check-the-reports" title="Permalink to this headline">#</a></h2>
<p>Print out the reports generated by Vivado HLS. Pay attention to the Utilization Estimates’ section in particular this time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hls4ml</span><span class="o">.</span><span class="n">report</span><span class="o">.</span><span class="n">read_vivado_report</span><span class="p">(</span><span class="s1">&#39;model_2/hls4ml_prj/&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Print the report for the model trained in part 1. Remember these models have the same architecture, but the model in this section was trained using the sparsity API from tensorflow_model_optimization. Notice how the resource usage had dramatically reduced (particularly the DSPs). When Vivado HLS notices an operation like <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">0</span> <span class="pre">*</span> <span class="pre">x</span></code> it can avoid placing a DSP for that operation. The impact of this is biggest when <code class="docutils literal notranslate"><span class="pre">ReuseFactor</span> <span class="pre">=</span> <span class="pre">1</span></code>, but still applies at higher reuse as well. <strong>Note you need to have trained and synthesized the model from part 1</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hls4ml</span><span class="o">.</span><span class="n">report</span><span class="o">.</span><span class="n">read_vivado_report</span><span class="p">(</span><span class="s1">&#39;model_1/hls4ml_prj&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="2.2_advanced_config.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Advanced hls4ml Configuration</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2.4_quantization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model Quantization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Javier Duarte, Dylan Rankin, and Patrick McCormack<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>