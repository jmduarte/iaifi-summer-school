
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Compression (Pruning) &#8212; IAIFI Summer School Tutorials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://jduarte.physics.ucsd.edu/iaifi-summer-school/2.3_compression.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Quantization" href="2.4_quantization.html" />
    <link rel="prev" title="Advanced Configuration" href="2.2_advanced_config.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">IAIFI Summer School Tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    IAIFI Summer School Tutorials
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Representations, Networks, and Symmetries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1.1_tabular_data_efps.html">
   Tabular Data using Energy Flow Polynomials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.2_jet_images.html">
   Jet Images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.3_deep_sets.html">
   Deep Sets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.4_gnn_in.html">
   Interaction Network GNN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.5_gnn_lorentz.html">
   Lorentz-Equivariant GNN
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model Compression and Fast Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="2.1_getting_started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2.2_advanced_config.html">
   Advanced Configuration
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Compression (Pruning)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2.4_quantization.html">
   Quantization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/jmduarte/iaifi-summer-school/main?urlpath=tree/book/2.3_compression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/jmduarte/iaifi-summer-school/blob/main/book/2.3_compression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jmduarte/iaifi-summer-school"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jmduarte/iaifi-summer-school/issues/new?title=Issue%20on%20page%20%2F2.3_compression.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/2.3_compression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-the-dataset-if-you-are-restarting-from-this-point">
   Load the dataset (if you are restarting from this point)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#construct-a-new-model">
   Construct a new model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-sparse">
   Train sparse
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-the-model">
   Train the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-sparsity">
   Check sparsity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-performance">
   Check performance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reduced-size-model">
     Reduced size model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Train the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Check sparsity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   Check performance
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Compression (Pruning)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-the-dataset-if-you-are-restarting-from-this-point">
   Load the dataset (if you are restarting from this point)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#construct-a-new-model">
   Construct a new model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-sparse">
   Train sparse
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-the-model">
   Train the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-sparsity">
   Check sparsity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-performance">
   Check performance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reduced-size-model">
     Reduced size model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Train the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Check sparsity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   Check performance
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>apt-get  -qq  install  -y  graphviz  <span class="o">&amp;&amp;</span>  pip  install  pydot
<span class="o">!</span>pip  install  -U  matplotlib
<span class="o">!</span>pip  install  git+https://github.com/fastmachinelearning/hls4ml.git@main#egg<span class="o">=</span>hls4ml<span class="o">[</span>profiling<span class="o">]</span>
<span class="o">!</span>pip  install  <span class="nv">qkeras</span><span class="o">==</span><span class="m">0</span>.9.0
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: matplotlib in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (3.5.2)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: python-dateutil&gt;=2.7 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: cycler&gt;=0.10 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib) (0.11.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib) (4.34.4)
Requirement already satisfied: pillow&gt;=6.2.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib) (9.2.0)
Requirement already satisfied: numpy&gt;=1.17 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib) (1.21.6)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib) (1.4.4)
Requirement already satisfied: pyparsing&gt;=2.2.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib) (3.0.9)
Requirement already satisfied: packaging&gt;=20.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib) (21.3)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: typing-extensions in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib) (4.3.0)
Requirement already satisfied: six&gt;=1.5 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting hls4ml[profiling]
  Cloning https://github.com/fastmachinelearning/hls4ml.git (to revision main) to /tmp/pip-install-yy4fb62y/hls4ml_af84c6753e8341789698e754a999bf1b
  Running command git clone --filter=blob:none --quiet https://github.com/fastmachinelearning/hls4ml.git /tmp/pip-install-yy4fb62y/hls4ml_af84c6753e8341789698e754a999bf1b
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  Resolved https://github.com/fastmachinelearning/hls4ml.git to commit 62046d799a4dbec150addc7f78fea5b579efeda1
  Running command git submodule update --init --recursive -q
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  Preparing metadata (setup.py) ... ?25l-
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> done
?25hRequirement already satisfied: numpy in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (1.21.6)
Requirement already satisfied: six in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (1.16.0)
Requirement already satisfied: pyyaml in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (6.0)
Requirement already satisfied: h5py in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (3.7.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: onnx&gt;=1.4.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (1.12.0)
Requirement already satisfied: calmjs.parse in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (1.3.0)
Requirement already satisfied: tabulate in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (0.8.10)
Requirement already satisfied: pydigitalwavetools==1.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (1.1)
Requirement already satisfied: qkeras in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (0.9.0)
Requirement already satisfied: pandas in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (1.3.5)
Requirement already satisfied: seaborn in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (0.11.2)
Requirement already satisfied: matplotlib in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from hls4ml[profiling]) (3.5.2)
Requirement already satisfied: typing-extensions&gt;=3.6.2.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from onnx&gt;=1.4.0-&gt;hls4ml[profiling]) (4.3.0)
Requirement already satisfied: protobuf&lt;=3.20.1,&gt;=3.12.2 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from onnx&gt;=1.4.0-&gt;hls4ml[profiling]) (3.19.4)
Requirement already satisfied: setuptools in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from calmjs.parse-&gt;hls4ml[profiling]) (63.3.0)
Requirement already satisfied: ply&gt;=3.6 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from calmjs.parse-&gt;hls4ml[profiling]) (3.11)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib-&gt;hls4ml[profiling]) (1.4.4)
Requirement already satisfied: pyparsing&gt;=2.2.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib-&gt;hls4ml[profiling]) (3.0.9)
Requirement already satisfied: pillow&gt;=6.2.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib-&gt;hls4ml[profiling]) (9.2.0)
Requirement already satisfied: python-dateutil&gt;=2.7 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib-&gt;hls4ml[profiling]) (2.8.2)
Requirement already satisfied: packaging&gt;=20.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib-&gt;hls4ml[profiling]) (21.3)
Requirement already satisfied: cycler&gt;=0.10 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib-&gt;hls4ml[profiling]) (0.11.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from matplotlib-&gt;hls4ml[profiling]) (4.34.4)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: pytz&gt;=2017.3 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from pandas-&gt;hls4ml[profiling]) (2022.1)
Requirement already satisfied: scikit-learn&gt;=0.23.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras-&gt;hls4ml[profiling]) (1.0.2)
Requirement already satisfied: pyparser in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras-&gt;hls4ml[profiling]) (1.0)
Requirement already satisfied: tqdm&gt;=4.48.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras-&gt;hls4ml[profiling]) (4.64.0)
Requirement already satisfied: networkx&gt;=2.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras-&gt;hls4ml[profiling]) (2.6.3)
Requirement already satisfied: tensorflow-model-optimization&gt;=0.2.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras-&gt;hls4ml[profiling]) (0.7.3)
Requirement already satisfied: scipy&gt;=1.4.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras-&gt;hls4ml[profiling]) (1.7.3)
Requirement already satisfied: keras-tuner&gt;=1.0.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras-&gt;hls4ml[profiling]) (1.1.3)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: tensorboard in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (2.9.1)
Requirement already satisfied: requests in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (2.28.1)
Requirement already satisfied: kt-legacy in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (1.0.4)
Requirement already satisfied: ipython in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (7.34.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: joblib&gt;=0.11 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from scikit-learn&gt;=0.23.1-&gt;qkeras-&gt;hls4ml[profiling]) (1.1.0)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from scikit-learn&gt;=0.23.1-&gt;qkeras-&gt;hls4ml[profiling]) (3.1.0)
Requirement already satisfied: dm-tree~=0.1.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorflow-model-optimization&gt;=0.2.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.1.7)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: parse==1.6.5 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from pyparser-&gt;qkeras-&gt;hls4ml[profiling]) (1.6.5)
Requirement already satisfied: decorator in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (5.1.1)
Requirement already satisfied: backcall in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.2.0)
Requirement already satisfied: pickleshare in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.7.5)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: pexpect&gt;4.3 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (4.8.0)
Requirement already satisfied: matplotlib-inline in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.1.3)
Requirement already satisfied: traitlets&gt;=4.2 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (5.3.0)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (3.0.30)
Requirement already satisfied: pygments in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (2.12.0)
Requirement already satisfied: jedi&gt;=0.16 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.18.1)
Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (2.1.0)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (3.3)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (1.26.11)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (2022.6.15)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (1.8.1)
Requirement already satisfied: grpcio&gt;=1.24.3 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (1.48.0)
Requirement already satisfied: absl-py&gt;=0.4 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (1.2.0)
Requirement already satisfied: wheel&gt;=0.26 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.37.1)
Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (2.9.1)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.6.1)
Requirement already satisfied: markdown&gt;=2.6.8 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (3.4.1)
Requirement already satisfied: werkzeug&gt;=1.0.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (2.2.1)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.4.6)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (5.2.0)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.2.8)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (4.9)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (1.3.1)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from jedi&gt;=0.16-&gt;ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.8.3)
Requirement already satisfied: importlib-metadata&gt;=4.4 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from markdown&gt;=2.6.8-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (4.12.0)
Requirement already satisfied: ptyprocess&gt;=0.5 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from pexpect&gt;4.3-&gt;ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.7.0)
Requirement already satisfied: wcwidth in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.2.5)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: MarkupSafe&gt;=2.1.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (2.1.1)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: zipp&gt;=0.5 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from importlib-metadata&gt;=4.4-&gt;markdown&gt;=2.6.8-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (3.8.1)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (0.4.8)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: oauthlib&gt;=3.0.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras-&gt;hls4ml[profiling]) (3.2.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: qkeras==0.9.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (0.9.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: setuptools&gt;=41.0.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras==0.9.0) (63.3.0)
Requirement already satisfied: tensorflow-model-optimization&gt;=0.2.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras==0.9.0) (0.7.3)
Requirement already satisfied: keras-tuner&gt;=1.0.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras==0.9.0) (1.1.3)
Requirement already satisfied: tqdm&gt;=4.48.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras==0.9.0) (4.64.0)
Requirement already satisfied: scipy&gt;=1.4.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras==0.9.0) (1.7.3)
Requirement already satisfied: scikit-learn&gt;=0.23.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras==0.9.0) (1.0.2)
Requirement already satisfied: pyparser in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras==0.9.0) (1.0)
Requirement already satisfied: numpy&gt;=1.16.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras==0.9.0) (1.21.6)
Requirement already satisfied: networkx&gt;=2.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from qkeras==0.9.0) (2.6.3)
Requirement already satisfied: kt-legacy in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (1.0.4)
Requirement already satisfied: requests in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (2.28.1)
Requirement already satisfied: ipython in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (7.34.0)
Requirement already satisfied: tensorboard in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (2.9.1)
Requirement already satisfied: packaging in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (21.3)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from scikit-learn&gt;=0.23.1-&gt;qkeras==0.9.0) (3.1.0)
Requirement already satisfied: joblib&gt;=0.11 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from scikit-learn&gt;=0.23.1-&gt;qkeras==0.9.0) (1.1.0)
Requirement already satisfied: dm-tree~=0.1.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorflow-model-optimization&gt;=0.2.1-&gt;qkeras==0.9.0) (0.1.7)
Requirement already satisfied: six~=1.10 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorflow-model-optimization&gt;=0.2.1-&gt;qkeras==0.9.0) (1.16.0)
Requirement already satisfied: parse==1.6.5 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from pyparser-&gt;qkeras==0.9.0) (1.6.5)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: matplotlib-inline in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.1.3)
Requirement already satisfied: traitlets&gt;=4.2 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (5.3.0)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (3.0.30)
Requirement already satisfied: pickleshare in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.7.5)
Requirement already satisfied: backcall in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.2.0)
Requirement already satisfied: pexpect&gt;4.3 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (4.8.0)
Requirement already satisfied: jedi&gt;=0.16 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.18.1)
Requirement already satisfied: decorator in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (5.1.1)
Requirement already satisfied: pygments in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (2.12.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from packaging-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (3.0.9)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (3.3)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (2022.6.15)
Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (2.1.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (1.26.11)
Requirement already satisfied: wheel&gt;=0.26 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.37.1)
Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (2.9.1)
Requirement already satisfied: protobuf&lt;3.20,&gt;=3.9.2 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (3.19.4)
Requirement already satisfied: markdown&gt;=2.6.8 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (3.4.1)
Requirement already satisfied: absl-py&gt;=0.4 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (1.2.0)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.6.1)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.4.6)
Requirement already satisfied: werkzeug&gt;=1.0.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (2.2.1)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (1.8.1)
Requirement already satisfied: grpcio&gt;=1.24.3 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (1.48.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.2.8)
Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (5.2.0)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (4.9)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (1.3.1)
Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from jedi&gt;=0.16-&gt;ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.8.3)
Requirement already satisfied: importlib-metadata&gt;=4.4 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from markdown&gt;=2.6.8-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (4.12.0)
Requirement already satisfied: ptyprocess&gt;=0.5 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from pexpect&gt;4.3-&gt;ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.7.0)
Requirement already satisfied: wcwidth in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.2.5)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: MarkupSafe&gt;=2.1.1 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (2.1.1)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: typing-extensions&gt;=3.6.4 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from importlib-metadata&gt;=4.4-&gt;markdown&gt;=2.6.8-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (4.3.0)
Requirement already satisfied: zipp&gt;=0.5 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from importlib-metadata&gt;=4.4-&gt;markdown&gt;=2.6.8-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (3.8.1)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (0.4.8)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard-&gt;keras-tuner&gt;=1.0.1-&gt;qkeras==0.9.0) (3.2.0)
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="compression-pruning">
<h1>Compression (Pruning)<a class="headerlink" href="#compression-pruning" title="Permalink to this headline">#</a></h1>
<section id="load-the-dataset-if-you-are-restarting-from-this-point">
<h2>Load the dataset (if you are restarting from this point)<a class="headerlink" href="#load-the-dataset-if-you-are-restarting-from-this-point" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="c1"># import os</span>
<span class="c1"># os.environ[&#39;PATH&#39;] = &#39;/opt/Xilinx/Vivado/2019.2/bin:&#39; + os.environ[&#39;PATH&#39;]</span>
<span class="c1"># for this tutorial we wont be actually running Vivado, so I have commented these lines out</span>
<span class="c1">#     but if you want to look into actually running on an FPGA then simply uncomment these lines</span>

<span class="n">X_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;X_train_val.npy&quot;</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;X_test.npy&quot;</span><span class="p">)</span>
<span class="n">y_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;y_train_val.npy&quot;</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;y_test.npy&quot;</span><span class="p">)</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;classes.npy&quot;</span><span class="p">,</span> <span class="n">allow_pickle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-08-01 14:40:22.138784: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.7.13/x64/lib
2022-08-01 14:40:22.138817: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_5609</span><span class="o">/</span><span class="mf">300696682.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span> <span class="c1">#     but if you want to look into actually running on an FPGA then simply uncomment these lines</span>
<span class="g g-Whitespace">     </span><span class="mi">18</span> 
<span class="ne">---&gt; </span><span class="mi">19</span> <span class="n">X_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;X_train_val.npy&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;X_test.npy&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span> <span class="n">y_train_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;y_train_val.npy&quot;</span><span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/numpy/lib/npyio.py</span> in <span class="ni">load</span><span class="nt">(file, mmap_mode, allow_pickle, fix_imports, encoding)</span>
<span class="g g-Whitespace">    </span><span class="mi">415</span>             <span class="n">own_fid</span> <span class="o">=</span> <span class="kc">False</span>
<span class="g g-Whitespace">    </span><span class="mi">416</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">417</span>             <span class="n">fid</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">os_fspath</span><span class="p">(</span><span class="n">file</span><span class="p">),</span> <span class="s2">&quot;rb&quot;</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">418</span>             <span class="n">own_fid</span> <span class="o">=</span> <span class="kc">True</span>
<span class="g g-Whitespace">    </span><span class="mi">419</span> 

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;X_train_val.npy&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="construct-a-new-model">
<h2>Construct a new model<a class="headerlink" href="#construct-a-new-model" title="Permalink to this headline">#</a></h2>
<p>We’ll now use the same architecture as we originally used: 3 hidden layers with 64, then 32, then 32 neurons. Each layer will use <code class="docutils literal notranslate"><span class="pre">relu</span></code> activation.
Add an output layer with 5 neurons (one for each class), then finish with Softmax activation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.regularizers</span> <span class="kn">import</span> <span class="n">l1</span>
<span class="kn">from</span> <span class="nn">callbacks</span> <span class="kn">import</span> <span class="n">all_callbacks</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,),</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;fc1&quot;</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;lecun_uniform&quot;</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;relu1&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">Dense</span><span class="p">(</span>
        <span class="mi">32</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;fc2&quot;</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;lecun_uniform&quot;</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;relu2&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">Dense</span><span class="p">(</span>
        <span class="mi">32</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;fc3&quot;</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;lecun_uniform&quot;</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;relu3&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">Dense</span><span class="p">(</span>
        <span class="mi">5</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;lecun_uniform&quot;</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-sparse">
<h2>Train sparse<a class="headerlink" href="#train-sparse" title="Permalink to this headline">#</a></h2>
<p>This time we’ll use the Tensorflow model optimization sparsity to train a sparse model (forcing many weights to ‘0’). In this instance, the target sparsity is 75%</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow_model_optimization.python.core.sparsity.keras</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">prune</span><span class="p">,</span>
    <span class="n">pruning_callbacks</span><span class="p">,</span>
    <span class="n">pruning_schedule</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">tensorflow_model_optimization.sparsity.keras</span> <span class="kn">import</span> <span class="n">strip_pruning</span>

<span class="n">pruning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;pruning_schedule&quot;</span><span class="p">:</span> <span class="n">pruning_schedule</span><span class="o">.</span><span class="n">ConstantSparsity</span><span class="p">(</span>
        <span class="mf">0.75</span><span class="p">,</span> <span class="n">begin_step</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">)</span>
<span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prune</span><span class="o">.</span><span class="n">prune_low_magnitude</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">pruning_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-the-model">
<h2>Train the model<a class="headerlink" href="#train-the-model" title="Permalink to this headline">#</a></h2>
<p>We’ll use the same settings as the previous model: Adam optimizer with categorical crossentropy loss.
The callbacks will decay the learning rate and save the model into a directory ‘model_3’
The model isn’t very complex, so this should just take a few minutes even on the CPU.
If you’ve restarted the notebook kernel after training once, set <code class="docutils literal notranslate"><span class="pre">train</span> <span class="pre">=</span> <span class="pre">False</span></code> to load the trained model rather than training again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">train</span><span class="p">:</span>
    <span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;categorical_crossentropy&quot;</span><span class="p">],</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">all_callbacks</span><span class="p">(</span>
        <span class="n">stop_patience</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">lr_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">lr_patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">lr_epsilon</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">,</span>
        <span class="n">lr_cooldown</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">lr_minimum</span><span class="o">=</span><span class="mf">0.0000001</span><span class="p">,</span>
        <span class="n">outputDir</span><span class="o">=</span><span class="s2">&quot;model_3&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pruning_callbacks</span><span class="o">.</span><span class="n">UpdatePruningStep</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">X_train_val</span><span class="p">,</span>
        <span class="n">y_train_val</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Save the model again but with the pruning &#39;stripped&#39; to use the regular layer types</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">strip_pruning</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;model_3/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model_3/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
Epoch 1/30
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  1/487 [..............................] - ETA: 26:10 - loss: 1.6388 - accuracy: 0.3027WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0623s). Check your callbacks.
477/487 [============================&gt;.] - ETA: 0s - loss: 1.3332 - accuracy: 0.5138
***callbacks***
saving losses to model_3/losses.log

Epoch 1: val_loss improved from inf to 1.12515, saving model to model_3/KERAS_check_best_model.h5

Epoch 1: val_loss improved from inf to 1.12515, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 1: saving model to model_3/KERAS_check_model_last.h5

Epoch 1: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 7s 7ms/step - loss: 1.3290 - accuracy: 0.5161 - val_loss: 1.1252 - val_accuracy: 0.6363 - lr: 1.0000e-04
Epoch 2/30
484/487 [============================&gt;.] - ETA: 0s - loss: 1.0553 - accuracy: 0.6667
***callbacks***
saving losses to model_3/losses.log

Epoch 2: val_loss improved from 1.12515 to 1.00705, saving model to model_3/KERAS_check_best_model.h5

Epoch 2: val_loss improved from 1.12515 to 1.00705, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 2: saving model to model_3/KERAS_check_model_last.h5

Epoch 2: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.0550 - accuracy: 0.6668 - val_loss: 1.0071 - val_accuracy: 0.6900 - lr: 1.0000e-04
Epoch 3/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.9714 - accuracy: 0.7004
***callbacks***
saving losses to model_3/losses.log

Epoch 3: val_loss improved from 1.00705 to 0.94499, saving model to model_3/KERAS_check_best_model.h5

Epoch 3: val_loss improved from 1.00705 to 0.94499, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 3: saving model to model_3/KERAS_check_model_last.h5

Epoch 3: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9712 - accuracy: 0.7005 - val_loss: 0.9450 - val_accuracy: 0.7098 - lr: 1.0000e-04
Epoch 4/30
485/487 [============================&gt;.] - ETA: 0s - loss: 0.9184 - accuracy: 0.7142
***callbacks***
saving losses to model_3/losses.log

Epoch 4: val_loss improved from 0.94499 to 0.90046, saving model to model_3/KERAS_check_best_model.h5

Epoch 4: val_loss improved from 0.94499 to 0.90046, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 4: saving model to model_3/KERAS_check_model_last.h5

Epoch 4: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9183 - accuracy: 0.7143 - val_loss: 0.9005 - val_accuracy: 0.7187 - lr: 1.0000e-04
Epoch 5/30
482/487 [============================&gt;.] - ETA: 0s - loss: 1.0756 - accuracy: 0.6411
***callbacks***
saving losses to model_3/losses.log

Epoch 5: val_loss did not improve from 0.90046

Epoch 5: val_loss did not improve from 0.90046

Epoch 5: saving model to model_3/KERAS_check_model_last.h5

Epoch 5: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.0749 - accuracy: 0.6415 - val_loss: 0.9990 - val_accuracy: 0.6722 - lr: 1.0000e-04
Epoch 6/30
487/487 [==============================] - ETA: 0s - loss: 0.9614 - accuracy: 0.6864
***callbacks***
saving losses to model_3/losses.log

Epoch 6: val_loss did not improve from 0.90046

Epoch 6: val_loss did not improve from 0.90046

Epoch 6: saving model to model_3/KERAS_check_model_last.h5

Epoch 6: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9614 - accuracy: 0.6864 - val_loss: 0.9364 - val_accuracy: 0.6951 - lr: 1.0000e-04
Epoch 7/30
487/487 [==============================] - ETA: 0s - loss: 0.9157 - accuracy: 0.7015
***callbacks***
saving losses to model_3/losses.log

Epoch 7: val_loss did not improve from 0.90046

Epoch 7: val_loss did not improve from 0.90046

Epoch 7: saving model to model_3/KERAS_check_model_last.h5

Epoch 7: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9157 - accuracy: 0.7015 - val_loss: 0.9021 - val_accuracy: 0.7052 - lr: 1.0000e-04
Epoch 8/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.8866 - accuracy: 0.7101
***callbacks***
saving losses to model_3/losses.log

Epoch 8: val_loss improved from 0.90046 to 0.87748, saving model to model_3/KERAS_check_best_model.h5

Epoch 8: val_loss improved from 0.90046 to 0.87748, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 8: saving model to model_3/KERAS_check_model_last.h5

Epoch 8: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8865 - accuracy: 0.7101 - val_loss: 0.8775 - val_accuracy: 0.7122 - lr: 1.0000e-04
Epoch 9/30
481/487 [============================&gt;.] - ETA: 0s - loss: 0.8658 - accuracy: 0.7155
***callbacks***
saving losses to model_3/losses.log

Epoch 9: val_loss improved from 0.87748 to 0.86037, saving model to model_3/KERAS_check_best_model.h5

Epoch 9: val_loss improved from 0.87748 to 0.86037, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 9: saving model to model_3/KERAS_check_model_last.h5

Epoch 9: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8655 - accuracy: 0.7156 - val_loss: 0.8604 - val_accuracy: 0.7173 - lr: 1.0000e-04
Epoch 10/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.8507 - accuracy: 0.7192
***callbacks***
saving losses to model_3/losses.log

Epoch 10: val_loss improved from 0.86037 to 0.84773, saving model to model_3/KERAS_check_best_model.h5

Epoch 10: val_loss improved from 0.86037 to 0.84773, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 10: saving model to model_3/KERAS_check_model_last.h5

Epoch 10: saving model to model_3/KERAS_check_model_last_weights.h5

Epoch 10: saving model to model_3/KERAS_check_model_epoch10.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8506 - accuracy: 0.7192 - val_loss: 0.8477 - val_accuracy: 0.7208 - lr: 1.0000e-04
Epoch 11/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.8392 - accuracy: 0.7219
***callbacks***
saving losses to model_3/losses.log

Epoch 11: val_loss improved from 0.84773 to 0.83780, saving model to model_3/KERAS_check_best_model.h5

Epoch 11: val_loss improved from 0.84773 to 0.83780, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 11: saving model to model_3/KERAS_check_model_last.h5

Epoch 11: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8392 - accuracy: 0.7219 - val_loss: 0.8378 - val_accuracy: 0.7228 - lr: 1.0000e-04
Epoch 12/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.8299 - accuracy: 0.7243
***callbacks***
saving losses to model_3/losses.log

Epoch 12: val_loss improved from 0.83780 to 0.82951, saving model to model_3/KERAS_check_best_model.h5

Epoch 12: val_loss improved from 0.83780 to 0.82951, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 12: saving model to model_3/KERAS_check_model_last.h5

Epoch 12: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8299 - accuracy: 0.7243 - val_loss: 0.8295 - val_accuracy: 0.7256 - lr: 1.0000e-04
Epoch 13/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.8219 - accuracy: 0.7264
***callbacks***
saving losses to model_3/losses.log

Epoch 13: val_loss improved from 0.82951 to 0.82224, saving model to model_3/KERAS_check_best_model.h5

Epoch 13: val_loss improved from 0.82951 to 0.82224, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 13: saving model to model_3/KERAS_check_model_last.h5

Epoch 13: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8220 - accuracy: 0.7263 - val_loss: 0.8222 - val_accuracy: 0.7270 - lr: 1.0000e-04
Epoch 14/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.8150 - accuracy: 0.7282
***callbacks***
saving losses to model_3/losses.log

Epoch 14: val_loss improved from 0.82224 to 0.81566, saving model to model_3/KERAS_check_best_model.h5

Epoch 14: val_loss improved from 0.82224 to 0.81566, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 14: saving model to model_3/KERAS_check_model_last.h5

Epoch 14: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8150 - accuracy: 0.7282 - val_loss: 0.8157 - val_accuracy: 0.7286 - lr: 1.0000e-04
Epoch 15/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.8087 - accuracy: 0.7298
***callbacks***
saving losses to model_3/losses.log

Epoch 15: val_loss improved from 0.81566 to 0.80984, saving model to model_3/KERAS_check_best_model.h5

Epoch 15: val_loss improved from 0.81566 to 0.80984, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 15: saving model to model_3/KERAS_check_model_last.h5

Epoch 15: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8087 - accuracy: 0.7299 - val_loss: 0.8098 - val_accuracy: 0.7301 - lr: 1.0000e-04
Epoch 16/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.8029 - accuracy: 0.7313
***callbacks***
saving losses to model_3/losses.log

Epoch 16: val_loss improved from 0.80984 to 0.80443, saving model to model_3/KERAS_check_best_model.h5

Epoch 16: val_loss improved from 0.80984 to 0.80443, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 16: saving model to model_3/KERAS_check_model_last.h5

Epoch 16: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8030 - accuracy: 0.7313 - val_loss: 0.8044 - val_accuracy: 0.7314 - lr: 1.0000e-04
Epoch 17/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.7977 - accuracy: 0.7325
***callbacks***
saving losses to model_3/losses.log

Epoch 17: val_loss improved from 0.80443 to 0.79965, saving model to model_3/KERAS_check_best_model.h5

Epoch 17: val_loss improved from 0.80443 to 0.79965, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 17: saving model to model_3/KERAS_check_model_last.h5

Epoch 17: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7978 - accuracy: 0.7325 - val_loss: 0.7997 - val_accuracy: 0.7324 - lr: 1.0000e-04
Epoch 18/30
479/487 [============================&gt;.] - ETA: 0s - loss: 0.7927 - accuracy: 0.7339
***callbacks***
saving losses to model_3/losses.log

Epoch 18: val_loss improved from 0.79965 to 0.79515, saving model to model_3/KERAS_check_best_model.h5

Epoch 18: val_loss improved from 0.79965 to 0.79515, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 18: saving model to model_3/KERAS_check_model_last.h5

Epoch 18: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7931 - accuracy: 0.7338 - val_loss: 0.7952 - val_accuracy: 0.7336 - lr: 1.0000e-04
Epoch 19/30
487/487 [==============================] - ETA: 0s - loss: 0.7887 - accuracy: 0.7348
***callbacks***
saving losses to model_3/losses.log

Epoch 19: val_loss improved from 0.79515 to 0.79111, saving model to model_3/KERAS_check_best_model.h5

Epoch 19: val_loss improved from 0.79515 to 0.79111, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 19: saving model to model_3/KERAS_check_model_last.h5

Epoch 19: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7887 - accuracy: 0.7348 - val_loss: 0.7911 - val_accuracy: 0.7345 - lr: 1.0000e-04
Epoch 20/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.7845 - accuracy: 0.7358
***callbacks***
saving losses to model_3/losses.log

Epoch 20: val_loss improved from 0.79111 to 0.78700, saving model to model_3/KERAS_check_best_model.h5

Epoch 20: val_loss improved from 0.79111 to 0.78700, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 20: saving model to model_3/KERAS_check_model_last.h5

Epoch 20: saving model to model_3/KERAS_check_model_last_weights.h5

Epoch 20: saving model to model_3/KERAS_check_model_epoch20.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7845 - accuracy: 0.7358 - val_loss: 0.7870 - val_accuracy: 0.7357 - lr: 1.0000e-04
Epoch 21/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.7808 - accuracy: 0.7369
***callbacks***
saving losses to model_3/losses.log

Epoch 21: val_loss improved from 0.78700 to 0.78325, saving model to model_3/KERAS_check_best_model.h5

Epoch 21: val_loss improved from 0.78700 to 0.78325, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 21: saving model to model_3/KERAS_check_model_last.h5

Epoch 21: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7806 - accuracy: 0.7369 - val_loss: 0.7833 - val_accuracy: 0.7368 - lr: 1.0000e-04
Epoch 22/30
484/487 [============================&gt;.] - ETA: 0s - loss: 0.7770 - accuracy: 0.7382
***callbacks***
saving losses to model_3/losses.log

Epoch 22: val_loss improved from 0.78325 to 0.77986, saving model to model_3/KERAS_check_best_model.h5

Epoch 22: val_loss improved from 0.78325 to 0.77986, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 22: saving model to model_3/KERAS_check_model_last.h5

Epoch 22: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7770 - accuracy: 0.7382 - val_loss: 0.7799 - val_accuracy: 0.7377 - lr: 1.0000e-04
Epoch 23/30
484/487 [============================&gt;.] - ETA: 0s - loss: 0.7738 - accuracy: 0.7393
***callbacks***
saving losses to model_3/losses.log

Epoch 23: val_loss improved from 0.77986 to 0.77674, saving model to model_3/KERAS_check_best_model.h5

Epoch 23: val_loss improved from 0.77986 to 0.77674, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 23: saving model to model_3/KERAS_check_model_last.h5

Epoch 23: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7738 - accuracy: 0.7393 - val_loss: 0.7767 - val_accuracy: 0.7385 - lr: 1.0000e-04
Epoch 24/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.7708 - accuracy: 0.7401
***callbacks***
saving losses to model_3/losses.log

Epoch 24: val_loss improved from 0.77674 to 0.77397, saving model to model_3/KERAS_check_best_model.h5

Epoch 24: val_loss improved from 0.77674 to 0.77397, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 24: saving model to model_3/KERAS_check_model_last.h5

Epoch 24: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7708 - accuracy: 0.7401 - val_loss: 0.7740 - val_accuracy: 0.7398 - lr: 1.0000e-04
Epoch 25/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.7681 - accuracy: 0.7410
***callbacks***
saving losses to model_3/losses.log

Epoch 25: val_loss improved from 0.77397 to 0.77153, saving model to model_3/KERAS_check_best_model.h5

Epoch 25: val_loss improved from 0.77397 to 0.77153, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 25: saving model to model_3/KERAS_check_model_last.h5

Epoch 25: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7681 - accuracy: 0.7410 - val_loss: 0.7715 - val_accuracy: 0.7403 - lr: 1.0000e-04
Epoch 26/30
487/487 [==============================] - ETA: 0s - loss: 0.7657 - accuracy: 0.7417
***callbacks***
saving losses to model_3/losses.log

Epoch 26: val_loss improved from 0.77153 to 0.76907, saving model to model_3/KERAS_check_best_model.h5

Epoch 26: val_loss improved from 0.77153 to 0.76907, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 26: saving model to model_3/KERAS_check_model_last.h5

Epoch 26: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 7ms/step - loss: 0.7657 - accuracy: 0.7417 - val_loss: 0.7691 - val_accuracy: 0.7413 - lr: 1.0000e-04
Epoch 27/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.7633 - accuracy: 0.7427
***callbacks***
saving losses to model_3/losses.log

Epoch 27: val_loss improved from 0.76907 to 0.76695, saving model to model_3/KERAS_check_best_model.h5

Epoch 27: val_loss improved from 0.76907 to 0.76695, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 27: saving model to model_3/KERAS_check_model_last.h5

Epoch 27: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7633 - accuracy: 0.7427 - val_loss: 0.7670 - val_accuracy: 0.7423 - lr: 1.0000e-04
Epoch 28/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.7612 - accuracy: 0.7432
***callbacks***
saving losses to model_3/losses.log

Epoch 28: val_loss improved from 0.76695 to 0.76481, saving model to model_3/KERAS_check_best_model.h5

Epoch 28: val_loss improved from 0.76695 to 0.76481, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 28: saving model to model_3/KERAS_check_model_last.h5

Epoch 28: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7611 - accuracy: 0.7433 - val_loss: 0.7648 - val_accuracy: 0.7424 - lr: 1.0000e-04
Epoch 29/30
481/487 [============================&gt;.] - ETA: 0s - loss: 0.7589 - accuracy: 0.7441
***callbacks***
saving losses to model_3/losses.log

Epoch 29: val_loss improved from 0.76481 to 0.76265, saving model to model_3/KERAS_check_best_model.h5

Epoch 29: val_loss improved from 0.76481 to 0.76265, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 29: saving model to model_3/KERAS_check_model_last.h5

Epoch 29: saving model to model_3/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7589 - accuracy: 0.7440 - val_loss: 0.7626 - val_accuracy: 0.7434 - lr: 1.0000e-04
Epoch 30/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.7571 - accuracy: 0.7446
***callbacks***
saving losses to model_3/losses.log

Epoch 30: val_loss improved from 0.76265 to 0.76078, saving model to model_3/KERAS_check_best_model.h5

Epoch 30: val_loss improved from 0.76265 to 0.76078, saving model to model_3/KERAS_check_best_model_weights.h5

Epoch 30: saving model to model_3/KERAS_check_model_last.h5

Epoch 30: saving model to model_3/KERAS_check_model_last_weights.h5

Epoch 30: saving model to model_3/KERAS_check_model_epoch30.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7570 - accuracy: 0.7446 - val_loss: 0.7608 - val_accuracy: 0.7439 - lr: 1.0000e-04
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
</pre></div>
</div>
</div>
</div>
</section>
<section id="check-sparsity">
<h2>Check sparsity<a class="headerlink" href="#check-sparsity" title="Permalink to this headline">#</a></h2>
<p>Make a quick check that the model was indeed trained sparse. We’ll just make a histogram of the weights of the 1st layer, and hopefully observe a large peak in the bin containing ‘0’. Note logarithmic y axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">h</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">b</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">h</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">% o</span><span class="s2">f zeros = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>% of zeros = 0.75
</pre></div>
</div>
<img alt="_images/2.3_compression_12_1.png" src="_images/2.3_compression_12_1.png" />
</div>
</div>
<p>Compare this to the first model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

<span class="n">model_orig</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model_1/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>

<span class="n">w_orig</span> <span class="o">=</span> <span class="n">model_orig</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">w_orig</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Original&quot;</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Pruned&quot;</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7ffb44373f90&gt;
</pre></div>
</div>
<img alt="_images/2.3_compression_14_1.png" src="_images/2.3_compression_14_1.png" />
</div>
</div>
</section>
<section id="check-performance">
<h2>Check performance<a class="headerlink" href="#check-performance" title="Permalink to this headline">#</a></h2>
<p>How does this 75% sparse model compare against the unpruned model? Let’s report the accuracy and make a ROC curve. The pruned model is shown with solid lines, the unpruned model is shown with dashed lines.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

<span class="n">model_ref</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model_1/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>

<span class="n">y_ref</span> <span class="o">=</span> <span class="n">model_ref</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prune</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Accuracy unpruned: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_ref</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Accuracy pruned:   </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_prune</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_ref</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prune</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>

<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">),</span> <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)]</span>
<span class="kn">from</span> <span class="nn">matplotlib.legend</span> <span class="kn">import</span> <span class="n">Legend</span>

<span class="n">leg</span> <span class="o">=</span> <span class="n">Legend</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">lines</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;unpruned&quot;</span><span class="p">,</span> <span class="s2">&quot;pruned&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">leg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy unpruned: 0.7516927710843373
Accuracy pruned:   0.7428975903614458
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7ffb3f857d10&gt;
</pre></div>
</div>
<img alt="_images/2.3_compression_16_2.png" src="_images/2.3_compression_16_2.png" />
</div>
</div>
<section id="reduced-size-model">
<h3>Reduced size model<a class="headerlink" href="#reduced-size-model" title="Permalink to this headline">#</a></h3>
<p>What if instead of pruning our model we simply shrink the size? Let’s now train a model where the hidden layers are a quarter of the size they are in the original model: 3 hidden layers with 16, then 8, then 8 neurons. Each layer will use <code class="docutils literal notranslate"><span class="pre">relu</span></code> activation.
Add an output layer with 5 neurons (one for each class), then finish with Softmax activation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_small</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model_small</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">Dense</span><span class="p">(</span>
        <span class="mi">16</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,),</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;fc1&quot;</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;lecun_uniform&quot;</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">model_small</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;relu1&quot;</span><span class="p">))</span>
<span class="n">model_small</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">Dense</span><span class="p">(</span>
        <span class="mi">8</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;fc2&quot;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;lecun_uniform&quot;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">model_small</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;relu2&quot;</span><span class="p">))</span>
<span class="n">model_small</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">Dense</span><span class="p">(</span>
        <span class="mi">8</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;fc3&quot;</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;lecun_uniform&quot;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">model_small</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;relu3&quot;</span><span class="p">))</span>
<span class="n">model_small</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">Dense</span><span class="p">(</span>
        <span class="mi">5</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;lecun_uniform&quot;</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">model_small</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">train</span><span class="p">:</span>
    <span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">model_small</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;categorical_crossentropy&quot;</span><span class="p">],</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">all_callbacks</span><span class="p">(</span>
        <span class="n">stop_patience</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">lr_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">lr_patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">lr_epsilon</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">,</span>
        <span class="n">lr_cooldown</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">lr_minimum</span><span class="o">=</span><span class="mf">0.0000001</span><span class="p">,</span>
        <span class="n">outputDir</span><span class="o">=</span><span class="s2">&quot;model_1_half&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model_small</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">X_train_val</span><span class="p">,</span>
        <span class="n">y_train_val</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model_small</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;model_1_small/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

    <span class="n">model_small</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model_1_small/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
  1/487 [..............................] - ETA: 4:42 - loss: 1.5291 - accuracy: 0.3262WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0042s). Check your callbacks.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>470/487 [===========================&gt;..] - ETA: 0s - loss: 1.4362 - accuracy: 0.3850
***callbacks***
saving losses to model_1_half/losses.log

Epoch 1: val_loss improved from inf to 1.34936, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 1: val_loss improved from inf to 1.34936, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 1: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 1: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 1.4334 - accuracy: 0.3867 - val_loss: 1.3494 - val_accuracy: 0.4396 - lr: 1.0000e-04
Epoch 2/30
476/487 [============================&gt;.] - ETA: 0s - loss: 1.2798 - accuracy: 0.5151
***callbacks***
saving losses to model_1_half/losses.log

Epoch 2: val_loss improved from 1.34936 to 1.21631, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 2: val_loss improved from 1.34936 to 1.21631, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 2: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 2: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 3ms/step - loss: 1.2784 - accuracy: 0.5160 - val_loss: 1.2163 - val_accuracy: 0.5594 - lr: 1.0000e-04
Epoch 3/30
469/487 [===========================&gt;..] - ETA: 0s - loss: 1.1709 - accuracy: 0.5623
***callbacks***
saving losses to model_1_half/losses.log

Epoch 3: val_loss improved from 1.21631 to 1.12769, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 3: val_loss improved from 1.21631 to 1.12769, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 3: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 3: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 3ms/step - loss: 1.1693 - accuracy: 0.5627 - val_loss: 1.1277 - val_accuracy: 0.5736 - lr: 1.0000e-04
Epoch 4/30
478/487 [============================&gt;.] - ETA: 0s - loss: 1.0857 - accuracy: 0.6013
***callbacks***
saving losses to model_1_half/losses.log

Epoch 4: val_loss improved from 1.12769 to 1.04812, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 4: val_loss improved from 1.12769 to 1.04812, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 4: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 4: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 1.0851 - accuracy: 0.6021 - val_loss: 1.0481 - val_accuracy: 0.6488 - lr: 1.0000e-04
Epoch 5/30
474/487 [============================&gt;.] - ETA: 0s - loss: 1.0160 - accuracy: 0.6735
***callbacks***
saving losses to model_1_half/losses.log

Epoch 5: val_loss improved from 1.04812 to 0.99132, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 5: val_loss improved from 1.04812 to 0.99132, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 5: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 5: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 3ms/step - loss: 1.0155 - accuracy: 0.6739 - val_loss: 0.9913 - val_accuracy: 0.6873 - lr: 1.0000e-04
Epoch 6/30
487/487 [==============================] - ETA: 0s - loss: 0.9674 - accuracy: 0.6938
***callbacks***
saving losses to model_1_half/losses.log

Epoch 6: val_loss improved from 0.99132 to 0.95099, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 6: val_loss improved from 0.99132 to 0.95099, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 6: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 6: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 3ms/step - loss: 0.9674 - accuracy: 0.6938 - val_loss: 0.9510 - val_accuracy: 0.6983 - lr: 1.0000e-04
Epoch 7/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.9316 - accuracy: 0.7022
***callbacks***
saving losses to model_1_half/losses.log

Epoch 7: val_loss improved from 0.95099 to 0.91891, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 7: val_loss improved from 0.95099 to 0.91891, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 7: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 7: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 3ms/step - loss: 0.9312 - accuracy: 0.7023 - val_loss: 0.9189 - val_accuracy: 0.7051 - lr: 1.0000e-04
Epoch 8/30
485/487 [============================&gt;.] - ETA: 0s - loss: 0.9022 - accuracy: 0.7086
***callbacks***
saving losses to model_1_half/losses.log

Epoch 8: val_loss improved from 0.91891 to 0.89320, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 8: val_loss improved from 0.91891 to 0.89320, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 8: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 8: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.9022 - accuracy: 0.7086 - val_loss: 0.8932 - val_accuracy: 0.7104 - lr: 1.0000e-04
Epoch 9/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.8787 - accuracy: 0.7128
***callbacks***
saving losses to model_1_half/losses.log

Epoch 9: val_loss improved from 0.89320 to 0.87132, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 9: val_loss improved from 0.89320 to 0.87132, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 9: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 9: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8788 - accuracy: 0.7128 - val_loss: 0.8713 - val_accuracy: 0.7144 - lr: 1.0000e-04
Epoch 10/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.8599 - accuracy: 0.7159
***callbacks***
saving losses to model_1_half/losses.log

Epoch 10: val_loss improved from 0.87132 to 0.85621, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 10: val_loss improved from 0.87132 to 0.85621, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 10: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 10: saving model to model_1_half/KERAS_check_model_last_weights.h5

Epoch 10: saving model to model_1_half/KERAS_check_model_epoch10.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8599 - accuracy: 0.7159 - val_loss: 0.8562 - val_accuracy: 0.7173 - lr: 1.0000e-04
Epoch 11/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.8471 - accuracy: 0.7182
***callbacks***
saving losses to model_1_half/losses.log

Epoch 11: val_loss improved from 0.85621 to 0.84522, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 11: val_loss improved from 0.85621 to 0.84522, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 11: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 11: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8470 - accuracy: 0.7183 - val_loss: 0.8452 - val_accuracy: 0.7187 - lr: 1.0000e-04
Epoch 12/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.8373 - accuracy: 0.7199
***callbacks***
saving losses to model_1_half/losses.log

Epoch 12: val_loss improved from 0.84522 to 0.83681, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 12: val_loss improved from 0.84522 to 0.83681, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 12: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 12: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8373 - accuracy: 0.7199 - val_loss: 0.8368 - val_accuracy: 0.7207 - lr: 1.0000e-04
Epoch 13/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.8297 - accuracy: 0.7212
***callbacks***
saving losses to model_1_half/losses.log

Epoch 13: val_loss improved from 0.83681 to 0.82991, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 13: val_loss improved from 0.83681 to 0.82991, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 13: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 13: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 4s 7ms/step - loss: 0.8296 - accuracy: 0.7212 - val_loss: 0.8299 - val_accuracy: 0.7218 - lr: 1.0000e-04
Epoch 14/30
473/487 [============================&gt;.] - ETA: 0s - loss: 0.8233 - accuracy: 0.7223
***callbacks***
saving losses to model_1_half/losses.log

Epoch 14: val_loss improved from 0.82991 to 0.82427, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 14: val_loss improved from 0.82991 to 0.82427, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 14: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 14: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.8234 - accuracy: 0.7223 - val_loss: 0.8243 - val_accuracy: 0.7225 - lr: 1.0000e-04
Epoch 15/30
474/487 [============================&gt;.] - ETA: 0s - loss: 0.8181 - accuracy: 0.7230
***callbacks***
saving losses to model_1_half/losses.log

Epoch 15: val_loss improved from 0.82427 to 0.81937, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 15: val_loss improved from 0.82427 to 0.81937, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 15: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 15: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 7ms/step - loss: 0.8182 - accuracy: 0.7230 - val_loss: 0.8194 - val_accuracy: 0.7234 - lr: 1.0000e-04
Epoch 16/30
487/487 [==============================] - ETA: 0s - loss: 0.8136 - accuracy: 0.7237
***callbacks***
saving losses to model_1_half/losses.log

Epoch 16: val_loss improved from 0.81937 to 0.81514, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 16: val_loss improved from 0.81937 to 0.81514, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 16: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 16: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 4s 8ms/step - loss: 0.8136 - accuracy: 0.7237 - val_loss: 0.8151 - val_accuracy: 0.7243 - lr: 1.0000e-04
Epoch 17/30
479/487 [============================&gt;.] - ETA: 0s - loss: 0.8095 - accuracy: 0.7245
***callbacks***
saving losses to model_1_half/losses.log

Epoch 17: val_loss improved from 0.81514 to 0.81146, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 17: val_loss improved from 0.81514 to 0.81146, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 17: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 17: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 7ms/step - loss: 0.8096 - accuracy: 0.7245 - val_loss: 0.8115 - val_accuracy: 0.7250 - lr: 1.0000e-04
Epoch 18/30
484/487 [============================&gt;.] - ETA: 0s - loss: 0.8058 - accuracy: 0.7251
***callbacks***
saving losses to model_1_half/losses.log

Epoch 18: val_loss improved from 0.81146 to 0.80808, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 18: val_loss improved from 0.81146 to 0.80808, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 18: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 18: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 4s 8ms/step - loss: 0.8060 - accuracy: 0.7251 - val_loss: 0.8081 - val_accuracy: 0.7253 - lr: 1.0000e-04
Epoch 19/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.8031 - accuracy: 0.7254
***callbacks***
saving losses to model_1_half/losses.log

Epoch 19: val_loss improved from 0.80808 to 0.80524, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 19: val_loss improved from 0.80808 to 0.80524, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 19: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 19: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 4s 8ms/step - loss: 0.8029 - accuracy: 0.7255 - val_loss: 0.8052 - val_accuracy: 0.7259 - lr: 1.0000e-04
Epoch 20/30
477/487 [============================&gt;.] - ETA: 0s - loss: 0.8000 - accuracy: 0.7262
***callbacks***
saving losses to model_1_half/losses.log

Epoch 20: val_loss improved from 0.80524 to 0.80243, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 20: val_loss improved from 0.80524 to 0.80243, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 20: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 20: saving model to model_1_half/KERAS_check_model_last_weights.h5

Epoch 20: saving model to model_1_half/KERAS_check_model_epoch20.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.8000 - accuracy: 0.7262 - val_loss: 0.8024 - val_accuracy: 0.7263 - lr: 1.0000e-04
Epoch 21/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.7975 - accuracy: 0.7269
***callbacks***
saving losses to model_1_half/losses.log

Epoch 21: val_loss improved from 0.80243 to 0.79994, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 21: val_loss improved from 0.80243 to 0.79994, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 21: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 21: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 4s 8ms/step - loss: 0.7975 - accuracy: 0.7270 - val_loss: 0.7999 - val_accuracy: 0.7268 - lr: 1.0000e-04
Epoch 22/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.7951 - accuracy: 0.7275
***callbacks***
saving losses to model_1_half/losses.log

Epoch 22: val_loss improved from 0.79994 to 0.79778, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 22: val_loss improved from 0.79994 to 0.79778, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 22: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 22: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 4s 9ms/step - loss: 0.7951 - accuracy: 0.7275 - val_loss: 0.7978 - val_accuracy: 0.7273 - lr: 1.0000e-04
Epoch 23/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.7931 - accuracy: 0.7279
***callbacks***
saving losses to model_1_half/losses.log

Epoch 23: val_loss improved from 0.79778 to 0.79573, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 23: val_loss improved from 0.79778 to 0.79573, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 23: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 23: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7930 - accuracy: 0.7280 - val_loss: 0.7957 - val_accuracy: 0.7280 - lr: 1.0000e-04
Epoch 24/30
487/487 [==============================] - ETA: 0s - loss: 0.7911 - accuracy: 0.7286
***callbacks***
saving losses to model_1_half/losses.log

Epoch 24: val_loss improved from 0.79573 to 0.79387, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 24: val_loss improved from 0.79573 to 0.79387, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 24: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 24: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 7ms/step - loss: 0.7911 - accuracy: 0.7286 - val_loss: 0.7939 - val_accuracy: 0.7284 - lr: 1.0000e-04
Epoch 25/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.7892 - accuracy: 0.7291
***callbacks***
saving losses to model_1_half/losses.log

Epoch 25: val_loss improved from 0.79387 to 0.79224, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 25: val_loss improved from 0.79387 to 0.79224, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 25: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 25: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 5ms/step - loss: 0.7892 - accuracy: 0.7291 - val_loss: 0.7922 - val_accuracy: 0.7287 - lr: 1.0000e-04
Epoch 26/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.7874 - accuracy: 0.7294
***callbacks***
saving losses to model_1_half/losses.log

Epoch 26: val_loss improved from 0.79224 to 0.79057, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 26: val_loss improved from 0.79224 to 0.79057, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 26: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 26: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 4s 7ms/step - loss: 0.7876 - accuracy: 0.7294 - val_loss: 0.7906 - val_accuracy: 0.7293 - lr: 1.0000e-04
Epoch 27/30
476/487 [============================&gt;.] - ETA: 0s - loss: 0.7862 - accuracy: 0.7299
***callbacks***
saving losses to model_1_half/losses.log

Epoch 27: val_loss improved from 0.79057 to 0.78915, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 27: val_loss improved from 0.79057 to 0.78915, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 27: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 27: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 7ms/step - loss: 0.7860 - accuracy: 0.7299 - val_loss: 0.7892 - val_accuracy: 0.7300 - lr: 1.0000e-04
Epoch 28/30
479/487 [============================&gt;.] - ETA: 0s - loss: 0.7844 - accuracy: 0.7305
***callbacks***
saving losses to model_1_half/losses.log

Epoch 28: val_loss improved from 0.78915 to 0.78767, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 28: val_loss improved from 0.78915 to 0.78767, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 28: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 28: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7845 - accuracy: 0.7304 - val_loss: 0.7877 - val_accuracy: 0.7299 - lr: 1.0000e-04
Epoch 29/30
471/487 [============================&gt;.] - ETA: 0s - loss: 0.7830 - accuracy: 0.7309
***callbacks***
saving losses to model_1_half/losses.log

Epoch 29: val_loss improved from 0.78767 to 0.78623, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 29: val_loss improved from 0.78767 to 0.78623, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 29: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 29: saving model to model_1_half/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 2s 5ms/step - loss: 0.7830 - accuracy: 0.7308 - val_loss: 0.7862 - val_accuracy: 0.7304 - lr: 1.0000e-04
Epoch 30/30
475/487 [============================&gt;.] - ETA: 0s - loss: 0.7819 - accuracy: 0.7311
***callbacks***
saving losses to model_1_half/losses.log

Epoch 30: val_loss improved from 0.78623 to 0.78488, saving model to model_1_half/KERAS_check_best_model.h5

Epoch 30: val_loss improved from 0.78623 to 0.78488, saving model to model_1_half/KERAS_check_best_model_weights.h5

Epoch 30: saving model to model_1_half/KERAS_check_model_last.h5

Epoch 30: saving model to model_1_half/KERAS_check_model_last_weights.h5

Epoch 30: saving model to model_1_half/KERAS_check_model_epoch30.h5

***callbacks end***

487/487 [==============================] - 2s 4ms/step - loss: 0.7816 - accuracy: 0.7312 - val_loss: 0.7849 - val_accuracy: 0.7310 - lr: 1.0000e-04
</pre></div>
</div>
</div>
</div>
<p>How does this small model compare in terms of performance?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

<span class="n">model_ref</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model_1/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>

<span class="n">y_ref</span> <span class="o">=</span> <span class="n">model_ref</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prune</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_small</span> <span class="o">=</span> <span class="n">model_small</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Accuracy unpruned: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_ref</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Accuracy pruned:   </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_prune</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Accuracy small:    </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_small</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_ref</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prune</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_small</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>

<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">),</span> <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">),</span> <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)]</span>
<span class="kn">from</span> <span class="nn">matplotlib.legend</span> <span class="kn">import</span> <span class="n">Legend</span>

<span class="n">leg</span> <span class="o">=</span> <span class="n">Legend</span><span class="p">(</span>
    <span class="n">ax</span><span class="p">,</span> <span class="n">lines</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;unpruned&quot;</span><span class="p">,</span> <span class="s2">&quot;pruned&quot;</span><span class="p">,</span> <span class="s2">&quot;small&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">leg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy unpruned: 0.7516927710843373
Accuracy pruned:   0.7456506024096385
Accuracy small:    0.7289518072289156
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7ffb468b5590&gt;
</pre></div>
</div>
<img alt="_images/2.3_compression_21_2.png" src="_images/2.3_compression_21_2.png" />
</div>
</div>
<p>This looks quite good. Can we go further? Let’s try a sparsity of 95%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow_model_optimization.python.core.sparsity.keras</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">prune</span><span class="p">,</span>
    <span class="n">pruning_callbacks</span><span class="p">,</span>
    <span class="n">pruning_schedule</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">tensorflow_model_optimization.sparsity.keras</span> <span class="kn">import</span> <span class="n">strip_pruning</span>

<span class="n">pruning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;pruning_schedule&quot;</span><span class="p">:</span> <span class="n">pruning_schedule</span><span class="o">.</span><span class="n">ConstantSparsity</span><span class="p">(</span>
        <span class="mf">0.95</span><span class="p">,</span> <span class="n">begin_step</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">)</span>
<span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prune</span><span class="o">.</span><span class="n">prune_low_magnitude</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">pruning_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id1">
<h2>Train the model<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<p>We’ll use the same settings as the model for part 1: Adam optimizer with categorical crossentropy loss.
The callbacks will decay the learning rate and save the model into a directory ‘model_2’
The model isn’t very complex, so this should just take a few minutes even on the CPU.
If you’ve restarted the notebook kernel after training once, set <code class="docutils literal notranslate"><span class="pre">train</span> <span class="pre">=</span> <span class="pre">False</span></code> to load the trained model rather than training again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">train</span><span class="p">:</span>
    <span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;categorical_crossentropy&quot;</span><span class="p">],</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">all_callbacks</span><span class="p">(</span>
        <span class="n">stop_patience</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">lr_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">lr_patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">lr_epsilon</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">,</span>
        <span class="n">lr_cooldown</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">lr_minimum</span><span class="o">=</span><span class="mf">0.0000001</span><span class="p">,</span>
        <span class="n">outputDir</span><span class="o">=</span><span class="s2">&quot;model_4&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pruning_callbacks</span><span class="o">.</span><span class="n">UpdatePruningStep</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">X_train_val</span><span class="p">,</span>
        <span class="n">y_train_val</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="o">.</span><span class="n">callbacks</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Save the model again but with the pruning &#39;stripped&#39; to use the regular layer types</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">strip_pruning</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;model_4/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model_4/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
Epoch 1/30
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super(Adam, self).__init__(name, **kwargs)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  1/487 [..............................] - ETA: 17:22 - loss: 0.7500 - accuracy: 0.7529WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0051s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.
482/487 [============================&gt;.] - ETA: 0s - loss: 0.7544 - accuracy: 0.7454
***callbacks***
saving losses to model_4/losses.log

Epoch 1: val_loss improved from inf to 0.75678, saving model to model_4/KERAS_check_best_model.h5

Epoch 1: val_loss improved from inf to 0.75678, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 1: saving model to model_4/KERAS_check_model_last.h5

Epoch 1: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 5s 6ms/step - loss: 0.7541 - accuracy: 0.7455 - val_loss: 0.7568 - val_accuracy: 0.7454 - lr: 1.0000e-04
Epoch 2/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.7504 - accuracy: 0.7468
***callbacks***
saving losses to model_4/losses.log

Epoch 2: val_loss improved from 0.75678 to 0.75333, saving model to model_4/KERAS_check_best_model.h5

Epoch 2: val_loss improved from 0.75678 to 0.75333, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 2: saving model to model_4/KERAS_check_model_last.h5

Epoch 2: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7505 - accuracy: 0.7468 - val_loss: 0.7533 - val_accuracy: 0.7468 - lr: 1.0000e-04
Epoch 3/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.7473 - accuracy: 0.7477
***callbacks***
saving losses to model_4/losses.log

Epoch 3: val_loss improved from 0.75333 to 0.75063, saving model to model_4/KERAS_check_best_model.h5

Epoch 3: val_loss improved from 0.75333 to 0.75063, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 3: saving model to model_4/KERAS_check_model_last.h5

Epoch 3: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7473 - accuracy: 0.7477 - val_loss: 0.7506 - val_accuracy: 0.7478 - lr: 1.0000e-04
Epoch 4/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.7445 - accuracy: 0.7487
***callbacks***
saving losses to model_4/losses.log

Epoch 4: val_loss improved from 0.75063 to 0.74790, saving model to model_4/KERAS_check_best_model.h5

Epoch 4: val_loss improved from 0.75063 to 0.74790, saving model to model_4/KERAS_check_best_model_weights.h5

Epoch 4: saving model to model_4/KERAS_check_model_last.h5

Epoch 4: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.7445 - accuracy: 0.7487 - val_loss: 0.7479 - val_accuracy: 0.7489 - lr: 1.0000e-04
Epoch 5/30
479/487 [============================&gt;.] - ETA: 0s - loss: 1.2867 - accuracy: 0.5367
***callbacks***
saving losses to model_4/losses.log

Epoch 5: val_loss did not improve from 0.74790

Epoch 5: val_loss did not improve from 0.74790

Epoch 5: saving model to model_4/KERAS_check_model_last.h5

Epoch 5: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 5ms/step - loss: 1.2865 - accuracy: 0.5369 - val_loss: 1.2808 - val_accuracy: 0.5486 - lr: 1.0000e-04
Epoch 6/30
485/487 [============================&gt;.] - ETA: 0s - loss: 1.2340 - accuracy: 0.5611
***callbacks***
saving losses to model_4/losses.log

Epoch 6: val_loss did not improve from 0.74790

Epoch 6: val_loss did not improve from 0.74790

Epoch 6: saving model to model_4/KERAS_check_model_last.h5

Epoch 6: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.2339 - accuracy: 0.5611 - val_loss: 1.1963 - val_accuracy: 0.5635 - lr: 1.0000e-04
Epoch 7/30
476/487 [============================&gt;.] - ETA: 0s - loss: 1.1660 - accuracy: 0.5707
***callbacks***
saving losses to model_4/losses.log

Epoch 7: val_loss did not improve from 0.74790

Epoch 7: val_loss did not improve from 0.74790

Epoch 7: saving model to model_4/KERAS_check_model_last.h5

Epoch 7: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.1655 - accuracy: 0.5708 - val_loss: 1.1432 - val_accuracy: 0.5714 - lr: 1.0000e-04
Epoch 8/30
482/487 [============================&gt;.] - ETA: 0s - loss: 1.1242 - accuracy: 0.5774
***callbacks***
saving losses to model_4/losses.log

Epoch 8: val_loss did not improve from 0.74790

Epoch 8: val_loss did not improve from 0.74790

Epoch 8: saving model to model_4/KERAS_check_model_last.h5

Epoch 8: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.1240 - accuracy: 0.5774 - val_loss: 1.1107 - val_accuracy: 0.5768 - lr: 1.0000e-04
Epoch 9/30
484/487 [============================&gt;.] - ETA: 0s - loss: 1.0962 - accuracy: 0.5821
***callbacks***
saving losses to model_4/losses.log

Epoch 9: val_loss did not improve from 0.74790

Epoch 9: val_loss did not improve from 0.74790

Epoch 9: saving model to model_4/KERAS_check_model_last.h5

Epoch 9: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 5ms/step - loss: 1.0961 - accuracy: 0.5822 - val_loss: 1.0869 - val_accuracy: 0.5809 - lr: 1.0000e-04
Epoch 10/30
485/487 [============================&gt;.] - ETA: 0s - loss: 1.0754 - accuracy: 0.5857
***callbacks***
saving losses to model_4/losses.log

Epoch 10: val_loss did not improve from 0.74790

Epoch 10: val_loss did not improve from 0.74790

Epoch 10: saving model to model_4/KERAS_check_model_last.h5

Epoch 10: saving model to model_4/KERAS_check_model_last_weights.h5

Epoch 10: saving model to model_4/KERAS_check_model_epoch10.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.0754 - accuracy: 0.5858 - val_loss: 1.0690 - val_accuracy: 0.5836 - lr: 1.0000e-04
Epoch 11/30
483/487 [============================&gt;.] - ETA: 0s - loss: 1.0592 - accuracy: 0.5880
***callbacks***
saving losses to model_4/losses.log

Epoch 11: val_loss did not improve from 0.74790

Epoch 11: val_loss did not improve from 0.74790

Epoch 11: saving model to model_4/KERAS_check_model_last.h5

Epoch 11: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.0592 - accuracy: 0.5880 - val_loss: 1.0543 - val_accuracy: 0.5860 - lr: 1.0000e-04
Epoch 12/30
486/487 [============================&gt;.] - ETA: 0s - loss: 1.0455 - accuracy: 0.5896
***callbacks***
saving losses to model_4/losses.log

Epoch 12: val_loss did not improve from 0.74790

Epoch 12: val_loss did not improve from 0.74790

Epoch 12: saving model to model_4/KERAS_check_model_last.h5

Epoch 12: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 5ms/step - loss: 1.0455 - accuracy: 0.5896 - val_loss: 1.0416 - val_accuracy: 0.5875 - lr: 1.0000e-04
Epoch 13/30
484/487 [============================&gt;.] - ETA: 0s - loss: 1.0330 - accuracy: 0.5906
***callbacks***
saving losses to model_4/losses.log

Epoch 13: val_loss did not improve from 0.74790

Epoch 13: val_loss did not improve from 0.74790

Epoch 13: saving model to model_4/KERAS_check_model_last.h5

Epoch 13: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 5ms/step - loss: 1.0329 - accuracy: 0.5907 - val_loss: 1.0290 - val_accuracy: 0.5889 - lr: 1.0000e-04
Epoch 14/30
480/487 [============================&gt;.] - ETA: 0s - loss: 1.0210 - accuracy: 0.5919
***callbacks***
saving losses to model_4/losses.log

Epoch 14: val_loss did not improve from 0.74790

Epoch 14: val_loss did not improve from 0.74790

Epoch 14: saving model to model_4/KERAS_check_model_last.h5

Epoch 14: saving model to model_4/KERAS_check_model_last_weights.h5

Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.0209 - accuracy: 0.5918 - val_loss: 1.0178 - val_accuracy: 0.5901 - lr: 1.0000e-04
Epoch 15/30
484/487 [============================&gt;.] - ETA: 0s - loss: 1.0126 - accuracy: 0.5927
***callbacks***
saving losses to model_4/losses.log

Epoch 15: val_loss did not improve from 0.74790

Epoch 15: val_loss did not improve from 0.74790

Epoch 15: saving model to model_4/KERAS_check_model_last.h5

Epoch 15: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.0126 - accuracy: 0.5928 - val_loss: 1.0123 - val_accuracy: 0.5905 - lr: 5.0000e-05
Epoch 16/30
479/487 [============================&gt;.] - ETA: 0s - loss: 1.0073 - accuracy: 0.5934
***callbacks***
saving losses to model_4/losses.log

Epoch 16: val_loss did not improve from 0.74790

Epoch 16: val_loss did not improve from 0.74790

Epoch 16: saving model to model_4/KERAS_check_model_last.h5

Epoch 16: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.0072 - accuracy: 0.5932 - val_loss: 1.0069 - val_accuracy: 0.5912 - lr: 5.0000e-05
Epoch 17/30
486/487 [============================&gt;.] - ETA: 0s - loss: 1.0019 - accuracy: 0.5938
***callbacks***
saving losses to model_4/losses.log

Epoch 17: val_loss did not improve from 0.74790

Epoch 17: val_loss did not improve from 0.74790

Epoch 17: saving model to model_4/KERAS_check_model_last.h5

Epoch 17: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 1.0018 - accuracy: 0.5938 - val_loss: 1.0015 - val_accuracy: 0.5916 - lr: 5.0000e-05
Epoch 18/30
485/487 [============================&gt;.] - ETA: 0s - loss: 0.9964 - accuracy: 0.5943
***callbacks***
saving losses to model_4/losses.log

Epoch 18: val_loss did not improve from 0.74790

Epoch 18: val_loss did not improve from 0.74790

Epoch 18: saving model to model_4/KERAS_check_model_last.h5

Epoch 18: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9965 - accuracy: 0.5942 - val_loss: 0.9963 - val_accuracy: 0.5921 - lr: 5.0000e-05
Epoch 19/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.9916 - accuracy: 0.5945
***callbacks***
saving losses to model_4/losses.log

Epoch 19: val_loss did not improve from 0.74790

Epoch 19: val_loss did not improve from 0.74790

Epoch 19: saving model to model_4/KERAS_check_model_last.h5

Epoch 19: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9914 - accuracy: 0.5946 - val_loss: 0.9912 - val_accuracy: 0.5927 - lr: 5.0000e-05
Epoch 20/30
486/487 [============================&gt;.] - ETA: 0s - loss: 0.9864 - accuracy: 0.5951
***callbacks***
saving losses to model_4/losses.log

Epoch 20: val_loss did not improve from 0.74790

Epoch 20: val_loss did not improve from 0.74790

Epoch 20: saving model to model_4/KERAS_check_model_last.h5

Epoch 20: saving model to model_4/KERAS_check_model_last_weights.h5

Epoch 20: saving model to model_4/KERAS_check_model_epoch20.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9864 - accuracy: 0.5951 - val_loss: 0.9864 - val_accuracy: 0.5931 - lr: 5.0000e-05
Epoch 21/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.9818 - accuracy: 0.5955
***callbacks***
saving losses to model_4/losses.log

Epoch 21: val_loss did not improve from 0.74790

Epoch 21: val_loss did not improve from 0.74790

Epoch 21: saving model to model_4/KERAS_check_model_last.h5

Epoch 21: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9817 - accuracy: 0.5955 - val_loss: 0.9818 - val_accuracy: 0.5934 - lr: 5.0000e-05
Epoch 22/30
485/487 [============================&gt;.] - ETA: 0s - loss: 0.9772 - accuracy: 0.5959
***callbacks***
saving losses to model_4/losses.log

Epoch 22: val_loss did not improve from 0.74790

Epoch 22: val_loss did not improve from 0.74790

Epoch 22: saving model to model_4/KERAS_check_model_last.h5

Epoch 22: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 5ms/step - loss: 0.9772 - accuracy: 0.5959 - val_loss: 0.9774 - val_accuracy: 0.5938 - lr: 5.0000e-05
Epoch 23/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.9730 - accuracy: 0.5962
***callbacks***
saving losses to model_4/losses.log

Epoch 23: val_loss did not improve from 0.74790

Epoch 23: val_loss did not improve from 0.74790

Epoch 23: saving model to model_4/KERAS_check_model_last.h5

Epoch 23: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 5ms/step - loss: 0.9728 - accuracy: 0.5963 - val_loss: 0.9731 - val_accuracy: 0.5944 - lr: 5.0000e-05
Epoch 24/30
480/487 [============================&gt;.] - ETA: 0s - loss: 0.9687 - accuracy: 0.5966
***callbacks***
saving losses to model_4/losses.log

Epoch 24: val_loss did not improve from 0.74790

Epoch 24: val_loss did not improve from 0.74790

Epoch 24: saving model to model_4/KERAS_check_model_last.h5

Epoch 24: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9687 - accuracy: 0.5966 - val_loss: 0.9691 - val_accuracy: 0.5947 - lr: 5.0000e-05
Epoch 25/30
485/487 [============================&gt;.] - ETA: 0s - loss: 0.9648 - accuracy: 0.5969
***callbacks***
saving losses to model_4/losses.log

Epoch 25: val_loss did not improve from 0.74790

Epoch 25: val_loss did not improve from 0.74790

Epoch 25: saving model to model_4/KERAS_check_model_last.h5

Epoch 25: saving model to model_4/KERAS_check_model_last_weights.h5

Epoch 25: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9647 - accuracy: 0.5970 - val_loss: 0.9652 - val_accuracy: 0.5951 - lr: 5.0000e-05
Epoch 26/30
481/487 [============================&gt;.] - ETA: 0s - loss: 0.9616 - accuracy: 0.5973
***callbacks***
saving losses to model_4/losses.log

Epoch 26: val_loss did not improve from 0.74790

Epoch 26: val_loss did not improve from 0.74790

Epoch 26: saving model to model_4/KERAS_check_model_last.h5

Epoch 26: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9616 - accuracy: 0.5973 - val_loss: 0.9630 - val_accuracy: 0.5953 - lr: 2.5000e-05
Epoch 27/30
478/487 [============================&gt;.] - ETA: 0s - loss: 0.9594 - accuracy: 0.5975
***callbacks***
saving losses to model_4/losses.log

Epoch 27: val_loss did not improve from 0.74790

Epoch 27: val_loss did not improve from 0.74790

Epoch 27: saving model to model_4/KERAS_check_model_last.h5

Epoch 27: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9593 - accuracy: 0.5977 - val_loss: 0.9606 - val_accuracy: 0.5958 - lr: 2.5000e-05
Epoch 28/30
482/487 [============================&gt;.] - ETA: 0s - loss: 0.9569 - accuracy: 0.5984
***callbacks***
saving losses to model_4/losses.log

Epoch 28: val_loss did not improve from 0.74790

Epoch 28: val_loss did not improve from 0.74790

Epoch 28: saving model to model_4/KERAS_check_model_last.h5

Epoch 28: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9570 - accuracy: 0.5984 - val_loss: 0.9583 - val_accuracy: 0.5969 - lr: 2.5000e-05
Epoch 29/30
483/487 [============================&gt;.] - ETA: 0s - loss: 0.9547 - accuracy: 0.5999
***callbacks***
saving losses to model_4/losses.log

Epoch 29: val_loss did not improve from 0.74790

Epoch 29: val_loss did not improve from 0.74790

Epoch 29: saving model to model_4/KERAS_check_model_last.h5

Epoch 29: saving model to model_4/KERAS_check_model_last_weights.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9547 - accuracy: 0.5999 - val_loss: 0.9561 - val_accuracy: 0.5984 - lr: 2.5000e-05
Epoch 30/30
487/487 [==============================] - ETA: 0s - loss: 0.9524 - accuracy: 0.6017
***callbacks***
saving losses to model_4/losses.log

Epoch 30: val_loss did not improve from 0.74790

Epoch 30: val_loss did not improve from 0.74790

Epoch 30: saving model to model_4/KERAS_check_model_last.h5

Epoch 30: saving model to model_4/KERAS_check_model_last_weights.h5

Epoch 30: saving model to model_4/KERAS_check_model_epoch30.h5

***callbacks end***

487/487 [==============================] - 3s 6ms/step - loss: 0.9524 - accuracy: 0.6017 - val_loss: 0.9538 - val_accuracy: 0.6002 - lr: 2.5000e-05
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h2>Check sparsity<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">h</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">b</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">h</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">% o</span><span class="s2">f zeros = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>% of zeros = 0.9501953125
</pre></div>
</div>
<img alt="_images/2.3_compression_27_1.png" src="_images/2.3_compression_27_1.png" />
</div>
</div>
</section>
<section id="id3">
<h2>Check performance<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h2>
<p>How does this 95% sparse model compare against the other models?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

<span class="n">model_ref</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model_1/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>
<span class="n">model_prune75</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;model_2/KERAS_check_best_model.h5&quot;</span><span class="p">)</span>

<span class="n">y_ref</span> <span class="o">=</span> <span class="n">model_ref</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prune75</span> <span class="o">=</span> <span class="n">model_prune75</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prune95</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Accuracy unpruned:     </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_ref</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Accuracy pruned (75%): </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_prune75</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Accuracy pruned (95%): </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_prune95</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_ref</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prune75</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># reset the colors</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">makeRoc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prune95</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>

<span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">),</span> <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">),</span> <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)]</span>
<span class="kn">from</span> <span class="nn">matplotlib.legend</span> <span class="kn">import</span> <span class="n">Legend</span>

<span class="n">leg</span> <span class="o">=</span> <span class="n">Legend</span><span class="p">(</span>
    <span class="n">ax</span><span class="p">,</span>
    <span class="n">lines</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;unpruned&quot;</span><span class="p">,</span> <span class="s2">&quot;pruned (75%)&quot;</span><span class="p">,</span> <span class="s2">&quot;pruned (95%)&quot;</span><span class="p">],</span>
    <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">,</span>
    <span class="n">frameon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">leg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy unpruned:     0.7516927710843373
Accuracy pruned (75%): 0.7506325301204819
Accuracy pruned (95%): 0.6019036144578314
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7ffb3d85e050&gt;
</pre></div>
</div>
<img alt="_images/2.3_compression_29_2.png" src="_images/2.3_compression_29_2.png" />
</div>
</div>
<p>Ok, clearly 95% is too sparse for this model (at least using this scheme). For some classes you see that the performance is not terrible, but overall the performance loss is quite substantial.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="2.2_advanced_config.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Advanced Configuration</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="2.4_quantization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Quantization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Javier Duarte, Dylan Rankin, and Patrick McCormack<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>